Experiment
xlm-roberta
xlm-roberta-base
turkish
Evaluation
2023-05-24 09:35:42.895 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:35:42.895 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:35:42.895 | INFO     | __main__:<module>:91 - results/xlm-roberta/turkish/no_augment/dev.tsv
2023-05-24 09:35:42.899 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:35:42.899 | INFO     | __main__:<module>:108 - 0.88 0.89 0.86 0.91 0.86 0.89
normal
oversampling
Testing turkish , augmentation: normal
2023-05-24 09:35:44.318945: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:35:44.357309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:35:44.948178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:35:45.470 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:35:45.471 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:34,  1.48it/s]  1%|▏         | 4/318 [00:00<00:51,  6.05it/s]  2%|▏         | 7/318 [00:00<00:31,  9.92it/s]  3%|▎         | 10/318 [00:01<00:23, 13.05it/s]  4%|▍         | 13/318 [00:01<00:19, 15.49it/s]  5%|▌         | 16/318 [00:01<00:17, 17.33it/s]  6%|▌         | 19/318 [00:01<00:15, 18.70it/s]  7%|▋         | 22/318 [00:01<00:15, 19.69it/s]  8%|▊         | 25/318 [00:01<00:14, 20.41it/s]  9%|▉         | 28/318 [00:01<00:13, 20.93it/s] 10%|▉         | 31/318 [00:02<00:13, 21.29it/s] 11%|█         | 34/318 [00:02<00:13, 21.53it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.71it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.85it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.94it/s] 14%|█▍        | 46/318 [00:02<00:12, 22.00it/s] 15%|█▌        | 49/318 [00:02<00:12, 22.05it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.07it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.10it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.11it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.12it/s] 20%|██        | 64/318 [00:03<00:11, 22.13it/s] 21%|██        | 67/318 [00:03<00:11, 22.14it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.14it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.14it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.14it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.14it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.14it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.14it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.14it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.14it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.14it/s] 31%|███       | 97/318 [00:05<00:09, 22.14it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.12it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.15it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.31it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.26it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.23it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.21it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.18it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.17it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.16it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.15it/s] 41%|████      | 130/318 [00:06<00:08, 22.14it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.15it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.14it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.15it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.14it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.14it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.14it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.14it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.15it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.15it/s] 50%|█████     | 160/318 [00:07<00:07, 22.15it/s] 51%|█████▏    | 163/318 [00:07<00:06, 22.14it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.14it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.14it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.14it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.14it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.14it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.15it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.11it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.12it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.13it/s] 61%|██████    | 193/318 [00:09<00:05, 22.13it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.14it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.14it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.14it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.14it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.14it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.12it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.30it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.21it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.19it/s] 70%|███████   | 223/318 [00:10<00:04, 22.18it/s] 71%|███████   | 226/318 [00:10<00:04, 22.17it/s] 72%|███████▏  | 229/318 [00:10<00:04, 22.17it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.17it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.13it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.13it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.14it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.14it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.15it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.15it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.15it/s] 81%|████████  | 256/318 [00:12<00:02, 22.14it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.15it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.16it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.16it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.13it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.14it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.15it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.15it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.16it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.16it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.12it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.14it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.14it/s] 93%|█████████▎| 295/318 [00:13<00:01, 22.15it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.15it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.16it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.16it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.16it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.16it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.16it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.16it/s]                                                 100%|██████████| 318/318 [00:14<00:00, 22.16it/s]100%|██████████| 318/318 [00:14<00:00, 21.21it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.9957, 'train_samples_per_second': 168.848, 'train_steps_per_second': 21.206, 'train_loss': 0.6303722933403351, 'epoch': 3.0}
Evaluation
2023-05-24 09:36:24.017 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:36:24.017 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:36:24.018 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/normal/dev.tsv
2023-05-24 09:36:24.021 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:36:24.021 | INFO     | __main__:<module>:108 - 0.84 0.84 0.82 0.85 0.82 0.84
Evaluation
2023-05-24 09:36:24.603 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:36:24.604 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:36:24.604 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/normal/test.tsv
2023-05-24 09:36:24.607 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:36:24.607 | INFO     | __main__:<module>:108 - 0.85 0.85 0.84 0.88 0.84 0.85
duygusal
oversampling
Testing turkish , augmentation: duygusal
2023-05-24 09:36:26.036279: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:36:26.074428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:36:26.654398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:36:27.159 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:36:27.160 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:37,  1.46it/s]  1%|▏         | 4/318 [00:00<00:52,  5.98it/s]  2%|▏         | 7/318 [00:00<00:31,  9.83it/s]  3%|▎         | 10/318 [00:01<00:23, 12.96it/s]  4%|▍         | 13/318 [00:01<00:19, 15.41it/s]  5%|▌         | 16/318 [00:01<00:17, 17.26it/s]  6%|▌         | 19/318 [00:01<00:16, 18.64it/s]  7%|▋         | 22/318 [00:01<00:15, 19.64it/s]  8%|▊         | 25/318 [00:01<00:14, 20.36it/s]  9%|▉         | 28/318 [00:01<00:13, 20.83it/s] 10%|▉         | 31/318 [00:02<00:13, 21.21it/s] 11%|█         | 34/318 [00:02<00:13, 21.47it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.66it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.80it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.90it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.96it/s] 15%|█▌        | 49/318 [00:02<00:12, 22.01it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.04it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.07it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.09it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.10it/s] 20%|██        | 64/318 [00:03<00:11, 22.08it/s] 21%|██        | 67/318 [00:03<00:11, 22.09it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.10it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.11it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.11it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.12it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.12it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.11it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.11it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.12it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.11it/s] 31%|███       | 97/318 [00:05<00:10, 22.07it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.09it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.10it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.26it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.22it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.14it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.14it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.13it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.12it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.12it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.12it/s] 41%|████      | 130/318 [00:06<00:08, 22.12it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.12it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.12it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.12it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.12it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.12it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.12it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.12it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.12it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.12it/s] 50%|█████     | 160/318 [00:07<00:07, 22.12it/s] 51%|█████▏    | 163/318 [00:08<00:07, 22.10it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.10it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.11it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.11it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.11it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.08it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.09it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.08it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.08it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.09it/s] 61%|██████    | 193/318 [00:09<00:05, 22.08it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.03it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.06it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.07it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.08it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.10it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.09it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.26it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.22it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.19it/s] 70%|███████   | 223/318 [00:10<00:04, 22.17it/s] 71%|███████   | 226/318 [00:10<00:04, 22.15it/s] 72%|███████▏  | 229/318 [00:10<00:04, 22.15it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.14it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.13it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.11it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.10it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.09it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.09it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.09it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.09it/s] 81%|████████  | 256/318 [00:12<00:02, 22.10it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.11it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.11it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.12it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.12it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.12it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.12it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.13it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.12it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.12it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.12it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.12it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.12it/s] 93%|█████████▎| 295/318 [00:13<00:01, 22.09it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.11it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.11it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.12it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.12it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.13it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.13it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.13it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.13it/s]100%|██████████| 318/318 [00:15<00:00, 21.17it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0287, 'train_samples_per_second': 168.477, 'train_steps_per_second': 21.159, 'train_loss': 0.6321219198358884, 'epoch': 3.0}
Evaluation
2023-05-24 09:37:04.371 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:37:04.371 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:37:04.371 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/duygusal/dev.tsv
2023-05-24 09:37:04.375 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:37:04.375 | INFO     | __main__:<module>:108 - 0.84 0.86 0.94 0.79 0.94 0.84
Evaluation
2023-05-24 09:37:04.961 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:37:04.961 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:37:04.961 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/duygusal/test.tsv
2023-05-24 09:37:04.965 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:37:04.965 | INFO     | __main__:<module>:108 - 0.81 0.83 0.91 0.78 0.91 0.82
propaganda
oversampling
Testing turkish , augmentation: propaganda
2023-05-24 09:37:06.326482: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:37:06.364035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:37:06.923119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:37:07.427 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:37:07.428 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:37,  1.46it/s]  1%|▏         | 4/318 [00:00<00:52,  5.98it/s]  2%|▏         | 7/318 [00:00<00:31,  9.81it/s]  3%|▎         | 10/318 [00:01<00:23, 12.93it/s]  4%|▍         | 13/318 [00:01<00:19, 15.37it/s]  5%|▌         | 16/318 [00:01<00:17, 17.22it/s]  6%|▌         | 19/318 [00:01<00:16, 18.60it/s]  7%|▋         | 22/318 [00:01<00:15, 19.60it/s]  8%|▊         | 25/318 [00:01<00:14, 20.28it/s]  9%|▉         | 28/318 [00:01<00:13, 20.81it/s] 10%|▉         | 31/318 [00:02<00:13, 21.19it/s] 11%|█         | 34/318 [00:02<00:13, 21.45it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.64it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.77it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.87it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.92it/s] 15%|█▌        | 49/318 [00:02<00:12, 21.97it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.00it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.03it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.04it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.05it/s] 20%|██        | 64/318 [00:03<00:11, 22.06it/s] 21%|██        | 67/318 [00:03<00:11, 22.06it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.06it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.07it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.07it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.07it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.07it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.06it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.06it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.06it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.06it/s] 31%|███       | 97/318 [00:05<00:10, 22.06it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.06it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.07it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.21it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.17it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.14it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.12it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.10it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.09it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.05it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.06it/s] 41%|████      | 130/318 [00:06<00:08, 22.06it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.06it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.07it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.07it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.03it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.04it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.05it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.05it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.06it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.06it/s] 50%|█████     | 160/318 [00:07<00:07, 22.06it/s] 51%|█████▏    | 163/318 [00:08<00:07, 22.07it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.06it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.06it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.06it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.06it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.06it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.06it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.06it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.07it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.03it/s] 61%|██████    | 193/318 [00:09<00:05, 22.05it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.05it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.05it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.05it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.06it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.06it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.07it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.21it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.17it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.10it/s] 70%|███████   | 223/318 [00:10<00:04, 22.09it/s] 71%|███████   | 226/318 [00:10<00:04, 22.08it/s] 72%|███████▏  | 229/318 [00:11<00:04, 22.08it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.07it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.07it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.06it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.06it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.06it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.07it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.07it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.08it/s] 81%|████████  | 256/318 [00:12<00:02, 22.05it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.09it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.09it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.08it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.08it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.08it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.08it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.08it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.09it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.09it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.09it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.10it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.03it/s] 93%|█████████▎| 295/318 [00:14<00:01, 22.05it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.06it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.07it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.08it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.07it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.08it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.09it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.09it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.09it/s]100%|██████████| 318/318 [00:15<00:00, 21.13it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0565, 'train_samples_per_second': 168.166, 'train_steps_per_second': 21.12, 'train_loss': 0.6406583486113159, 'epoch': 3.0}
Evaluation
2023-05-24 09:37:46.363 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:37:46.364 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:37:46.364 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/propaganda/dev.tsv
2023-05-24 09:37:46.367 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:37:46.367 | INFO     | __main__:<module>:108 - 0.86 0.88 0.96 0.80 0.96 0.86
Evaluation
2023-05-24 09:37:46.922 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:37:46.922 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:37:46.923 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/propaganda/test.tsv
2023-05-24 09:37:46.926 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:37:46.926 | INFO     | __main__:<module>:108 - 0.85 0.87 0.94 0.82 0.94 0.85
öznel
oversampling
Testing turkish , augmentation: öznel
2023-05-24 09:37:48.356034: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:37:48.394547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:37:48.976837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:37:49.484 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:37:49.485 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:36,  1.47it/s]  1%|▏         | 4/318 [00:00<00:52,  6.01it/s]  2%|▏         | 7/318 [00:00<00:31,  9.86it/s]  3%|▎         | 10/318 [00:01<00:23, 12.99it/s]  4%|▍         | 13/318 [00:01<00:19, 15.43it/s]  5%|▌         | 16/318 [00:01<00:17, 17.28it/s]  6%|▌         | 19/318 [00:01<00:16, 18.65it/s]  7%|▋         | 22/318 [00:01<00:15, 19.65it/s]  8%|▊         | 25/318 [00:01<00:14, 20.37it/s]  9%|▉         | 28/318 [00:01<00:13, 20.89it/s] 10%|▉         | 31/318 [00:02<00:13, 21.25it/s] 11%|█         | 34/318 [00:02<00:13, 21.50it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.68it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.81it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.90it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.96it/s] 15%|█▌        | 49/318 [00:02<00:12, 22.01it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.05it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.06it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.08it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.09it/s] 20%|██        | 64/318 [00:03<00:11, 22.10it/s] 21%|██        | 67/318 [00:03<00:11, 22.10it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.10it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.11it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.10it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.10it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.10it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.11it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.10it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.11it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.11it/s] 31%|███       | 97/318 [00:05<00:09, 22.11it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.11it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.10it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.25it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.21it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.18it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.16it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.14it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.12it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.11it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.10it/s] 41%|████      | 130/318 [00:06<00:08, 22.10it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.11it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.11it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.11it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.11it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.11it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.10it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.12it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.12it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.12it/s] 50%|█████     | 160/318 [00:07<00:07, 22.12it/s] 51%|█████▏    | 163/318 [00:08<00:07, 22.12it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.12it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.12it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.11it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.10it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.10it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.10it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.05it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.07it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.08it/s] 61%|██████    | 193/318 [00:09<00:05, 22.09it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.10it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.10it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.09it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.10it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.10it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.11it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.25it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.21it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.18it/s] 70%|███████   | 223/318 [00:10<00:04, 22.16it/s] 71%|███████   | 226/318 [00:10<00:04, 22.15it/s] 72%|███████▏  | 229/318 [00:10<00:04, 22.14it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.10it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.10it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.09it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.09it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.10it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.10it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.10it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.09it/s] 81%|████████  | 256/318 [00:12<00:02, 22.10it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.10it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.10it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.07it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.08it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.09it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.09it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.09it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.10it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.06it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.07it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.08it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.09it/s] 93%|█████████▎| 295/318 [00:13<00:01, 22.09it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.09it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.10it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.10it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.10it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.10it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.11it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.05it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.05it/s]100%|██████████| 318/318 [00:15<00:00, 21.17it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0293, 'train_samples_per_second': 168.47, 'train_steps_per_second': 21.159, 'train_loss': 0.6289476118747543, 'epoch': 3.0}
Evaluation
2023-05-24 09:38:27.750 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:38:27.750 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:38:27.750 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/öznel/dev.tsv
2023-05-24 09:38:27.753 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:38:27.753 | INFO     | __main__:<module>:108 - 0.85 0.87 0.96 0.79 0.96 0.85
Evaluation
2023-05-24 09:38:28.344 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:38:28.344 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:38:28.345 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/öznel/test.tsv
2023-05-24 09:38:28.348 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:38:28.348 | INFO     | __main__:<module>:108 - 0.79 0.82 0.92 0.76 0.92 0.80
abartılı
oversampling
Testing turkish , augmentation: abartılı
2023-05-24 09:38:29.759852: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:38:29.797435: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:38:30.356727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:38:30.862 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:38:30.863 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:36,  1.47it/s]  1%|▏         | 4/318 [00:00<00:52,  6.00it/s]  2%|▏         | 7/318 [00:00<00:31,  9.86it/s]  3%|▎         | 10/318 [00:01<00:23, 12.98it/s]  4%|▍         | 13/318 [00:01<00:19, 15.42it/s]  5%|▌         | 16/318 [00:01<00:17, 17.27it/s]  6%|▌         | 19/318 [00:01<00:16, 18.64it/s]  7%|▋         | 22/318 [00:01<00:15, 19.63it/s]  8%|▊         | 25/318 [00:01<00:14, 20.35it/s]  9%|▉         | 28/318 [00:01<00:13, 20.86it/s] 10%|▉         | 31/318 [00:02<00:13, 21.23it/s] 11%|█         | 34/318 [00:02<00:13, 21.49it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.67it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.79it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.88it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.95it/s] 15%|█▌        | 49/318 [00:02<00:12, 21.99it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.02it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.01it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.04it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.06it/s] 20%|██        | 64/318 [00:03<00:11, 22.06it/s] 21%|██        | 67/318 [00:03<00:11, 22.07it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.08it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.08it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.08it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.08it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.08it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.08it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.08it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.09it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.09it/s] 31%|███       | 97/318 [00:05<00:10, 22.09it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.09it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.09it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.25it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.20it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.17it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.14it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.13it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.11it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.10it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.09it/s] 41%|████      | 130/318 [00:06<00:08, 22.08it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.06it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.06it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.06it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.05it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.05it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.06it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.07it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.06it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.08it/s] 50%|█████     | 160/318 [00:07<00:07, 22.09it/s] 51%|█████▏    | 163/318 [00:08<00:07, 22.08it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.08it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.08it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.05it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.06it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.06it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.07it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.08it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.08it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.05it/s] 61%|██████    | 193/318 [00:09<00:05, 22.06it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.07it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.08it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.08it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.08it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.08it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.08it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.24it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.19it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.16it/s] 70%|███████   | 223/318 [00:10<00:04, 22.11it/s] 71%|███████   | 226/318 [00:10<00:04, 22.10it/s] 72%|███████▏  | 229/318 [00:11<00:04, 22.10it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.10it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.10it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.08it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.08it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.08it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.09it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.08it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.08it/s] 81%|████████  | 256/318 [00:12<00:02, 22.08it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.08it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.08it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.08it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.08it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.04it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.05it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.07it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.07it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.08it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.08it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.08it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.08it/s] 93%|█████████▎| 295/318 [00:13<00:01, 22.08it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.08it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.08it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.02it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.04it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.05it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.06it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.05it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.05it/s]100%|██████████| 318/318 [00:15<00:00, 21.15it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0452, 'train_samples_per_second': 168.293, 'train_steps_per_second': 21.136, 'train_loss': 0.6276703840531643, 'epoch': 3.0}
Evaluation
2023-05-24 09:39:08.949 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:39:08.950 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:39:08.950 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/abartılı/dev.tsv
2023-05-24 09:39:08.953 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:39:08.953 | INFO     | __main__:<module>:108 - 0.85 0.87 0.96 0.79 0.96 0.85
Evaluation
2023-05-24 09:39:09.565 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:39:09.566 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:39:09.566 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/abartılı/test.tsv
2023-05-24 09:39:09.569 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:39:09.569 | INFO     | __main__:<module>:108 - 0.82 0.85 0.95 0.78 0.95 0.83
aşağılayıcı
oversampling
Testing turkish , augmentation: aşağılayıcı
2023-05-24 09:39:10.996257: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:39:11.034461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:39:11.615766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:39:12.122 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:39:12.123 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:35,  1.47it/s]  1%|▏         | 4/318 [00:00<00:52,  6.01it/s]  2%|▏         | 7/318 [00:00<00:31,  9.86it/s]  3%|▎         | 10/318 [00:01<00:23, 12.94it/s]  4%|▍         | 13/318 [00:01<00:19, 15.38it/s]  5%|▌         | 16/318 [00:01<00:17, 17.23it/s]  6%|▌         | 19/318 [00:01<00:16, 18.60it/s]  7%|▋         | 22/318 [00:01<00:15, 19.59it/s]  8%|▊         | 25/318 [00:01<00:14, 20.32it/s]  9%|▉         | 28/318 [00:01<00:13, 20.77it/s] 10%|▉         | 31/318 [00:02<00:13, 21.15it/s] 11%|█         | 34/318 [00:02<00:13, 21.42it/s] 12%|█▏        | 37/318 [00:02<00:13, 21.62it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.75it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.84it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.90it/s] 15%|█▌        | 49/318 [00:02<00:12, 21.95it/s] 16%|█▋        | 52/318 [00:02<00:12, 21.99it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.00it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.02it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.03it/s] 20%|██        | 64/318 [00:03<00:11, 22.04it/s] 21%|██        | 67/318 [00:03<00:11, 22.05it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.05it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.05it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.05it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.05it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.05it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.05it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.05it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.05it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.05it/s] 31%|███       | 97/318 [00:05<00:10, 22.06it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.05it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.06it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.22it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.17it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.13it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.11it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.09it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.08it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.08it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.03it/s] 41%|████      | 130/318 [00:06<00:08, 22.04it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.04it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.04it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.05it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.05it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.05it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.05it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.05it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.05it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.05it/s] 50%|█████     | 160/318 [00:07<00:07, 22.01it/s] 51%|█████▏    | 163/318 [00:08<00:07, 22.02it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.03it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.03it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.04it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.04it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.05it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.05it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.05it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.05it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.05it/s] 61%|██████    | 193/318 [00:09<00:05, 22.01it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.03it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.03it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.03it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.04it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.05it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.05it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.21it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.16it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.13it/s] 70%|███████   | 223/318 [00:10<00:04, 22.10it/s] 71%|███████   | 226/318 [00:10<00:04, 22.09it/s] 72%|███████▏  | 229/318 [00:11<00:04, 22.08it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.07it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.07it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.05it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.05it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.04it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.04it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.05it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.04it/s] 81%|████████  | 256/318 [00:12<00:02, 22.04it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.04it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.05it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.05it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.05it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.06it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.06it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.04it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.04it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.04it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.05it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.05it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.05it/s] 93%|█████████▎| 295/318 [00:14<00:01, 22.05it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.05it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.05it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.05it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.05it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.02it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.03it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.04it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.04it/s]100%|██████████| 318/318 [00:15<00:00, 21.12it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0642, 'train_samples_per_second': 168.081, 'train_steps_per_second': 21.11, 'train_loss': 0.6282354870682243, 'epoch': 3.0}
Evaluation
2023-05-24 09:39:50.400 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:39:50.400 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:39:50.400 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/aşağılayıcı/dev.tsv
2023-05-24 09:39:50.404 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:39:50.404 | INFO     | __main__:<module>:108 - 0.85 0.87 0.95 0.80 0.95 0.85
Evaluation
2023-05-24 09:39:50.988 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:39:50.989 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:39:50.989 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/aşağılayıcı/test.tsv
2023-05-24 09:39:50.992 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:39:50.992 | INFO     | __main__:<module>:108 - 0.83 0.85 0.95 0.79 0.95 0.83
partizan
oversampling
Testing turkish , augmentation: partizan
2023-05-24 09:39:52.415607: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:39:52.453954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:39:53.033873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:39:53.538 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:39:53.539 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:37,  1.46it/s]  1%|▏         | 4/318 [00:00<00:52,  5.99it/s]  2%|▏         | 7/318 [00:00<00:31,  9.84it/s]  3%|▎         | 10/318 [00:01<00:23, 12.94it/s]  4%|▍         | 13/318 [00:01<00:19, 15.39it/s]  5%|▌         | 16/318 [00:01<00:17, 17.26it/s]  6%|▌         | 19/318 [00:01<00:16, 18.65it/s]  7%|▋         | 22/318 [00:01<00:15, 19.65it/s]  8%|▊         | 25/318 [00:01<00:14, 20.39it/s]  9%|▉         | 28/318 [00:01<00:13, 20.91it/s] 10%|▉         | 31/318 [00:02<00:13, 21.27it/s] 11%|█         | 34/318 [00:02<00:13, 21.53it/s] 12%|█▏        | 37/318 [00:02<00:12, 21.71it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.85it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.94it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.96it/s] 15%|█▌        | 49/318 [00:02<00:12, 22.01it/s] 16%|█▋        | 52/318 [00:02<00:12, 22.05it/s] 17%|█▋        | 55/318 [00:03<00:11, 22.08it/s] 18%|█▊        | 58/318 [00:03<00:11, 22.10it/s] 19%|█▉        | 61/318 [00:03<00:11, 22.07it/s] 20%|██        | 64/318 [00:03<00:11, 22.09it/s] 21%|██        | 67/318 [00:03<00:11, 22.11it/s] 22%|██▏       | 70/318 [00:03<00:11, 22.12it/s] 23%|██▎       | 73/318 [00:03<00:11, 22.13it/s] 24%|██▍       | 76/318 [00:04<00:10, 22.13it/s] 25%|██▍       | 79/318 [00:04<00:10, 22.13it/s] 26%|██▌       | 82/318 [00:04<00:10, 22.13it/s] 27%|██▋       | 85/318 [00:04<00:10, 22.13it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.14it/s] 29%|██▊       | 91/318 [00:04<00:10, 22.14it/s] 30%|██▉       | 94/318 [00:04<00:10, 22.13it/s] 31%|███       | 97/318 [00:05<00:09, 22.13it/s] 31%|███▏      | 100/318 [00:05<00:09, 22.14it/s] 32%|███▏      | 103/318 [00:05<00:09, 22.13it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.29it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.25it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.20it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.18it/s] 37%|███▋      | 118/318 [00:05<00:09, 22.17it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.16it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.15it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.15it/s] 41%|████      | 130/318 [00:06<00:08, 22.14it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.14it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.14it/s] 44%|████▎     | 139/318 [00:06<00:08, 22.14it/s] 45%|████▍     | 142/318 [00:07<00:07, 22.14it/s] 46%|████▌     | 145/318 [00:07<00:07, 22.14it/s] 47%|████▋     | 148/318 [00:07<00:07, 22.13it/s] 47%|████▋     | 151/318 [00:07<00:07, 22.14it/s] 48%|████▊     | 154/318 [00:07<00:07, 22.14it/s] 49%|████▉     | 157/318 [00:07<00:07, 22.14it/s] 50%|█████     | 160/318 [00:07<00:07, 22.11it/s] 51%|█████▏    | 163/318 [00:08<00:06, 22.15it/s] 52%|█████▏    | 166/318 [00:08<00:06, 22.14it/s] 53%|█████▎    | 169/318 [00:08<00:06, 22.14it/s] 54%|█████▍    | 172/318 [00:08<00:06, 22.14it/s] 55%|█████▌    | 175/318 [00:08<00:06, 22.14it/s] 56%|█████▌    | 178/318 [00:08<00:06, 22.10it/s] 57%|█████▋    | 181/318 [00:08<00:06, 22.11it/s] 58%|█████▊    | 184/318 [00:08<00:06, 22.12it/s] 59%|█████▉    | 187/318 [00:09<00:05, 22.13it/s] 60%|█████▉    | 190/318 [00:09<00:05, 22.13it/s] 61%|██████    | 193/318 [00:09<00:05, 22.13it/s] 62%|██████▏   | 196/318 [00:09<00:05, 22.10it/s] 63%|██████▎   | 199/318 [00:09<00:05, 22.11it/s] 64%|██████▎   | 202/318 [00:09<00:05, 22.12it/s] 64%|██████▍   | 205/318 [00:09<00:05, 22.12it/s] 65%|██████▌   | 208/318 [00:10<00:04, 22.11it/s] 66%|██████▋   | 211/318 [00:10<00:04, 22.12it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.28it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.24it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.21it/s] 70%|███████   | 223/318 [00:10<00:04, 22.19it/s] 71%|███████   | 226/318 [00:10<00:04, 22.17it/s] 72%|███████▏  | 229/318 [00:10<00:04, 22.16it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.15it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.15it/s] 75%|███████▍  | 238/318 [00:11<00:03, 22.13it/s] 76%|███████▌  | 241/318 [00:11<00:03, 22.13it/s] 77%|███████▋  | 244/318 [00:11<00:03, 22.14it/s] 78%|███████▊  | 247/318 [00:11<00:03, 22.14it/s] 79%|███████▊  | 250/318 [00:11<00:03, 22.14it/s] 80%|███████▉  | 253/318 [00:12<00:02, 22.14it/s] 81%|████████  | 256/318 [00:12<00:02, 22.14it/s] 81%|████████▏ | 259/318 [00:12<00:02, 22.14it/s] 82%|████████▏ | 262/318 [00:12<00:02, 22.14it/s] 83%|████████▎ | 265/318 [00:12<00:02, 22.14it/s] 84%|████████▍ | 268/318 [00:12<00:02, 22.15it/s] 85%|████████▌ | 271/318 [00:12<00:02, 22.15it/s] 86%|████████▌ | 274/318 [00:13<00:01, 22.14it/s] 87%|████████▋ | 277/318 [00:13<00:01, 22.11it/s] 88%|████████▊ | 280/318 [00:13<00:01, 22.12it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.13it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.13it/s] 91%|█████████ | 289/318 [00:13<00:01, 22.14it/s] 92%|█████████▏| 292/318 [00:13<00:01, 22.14it/s] 93%|█████████▎| 295/318 [00:13<00:01, 22.14it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.14it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.14it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.15it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.15it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.10it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.12it/s] 99%|█████████▉| 316/318 [00:14<00:00, 22.12it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.12it/s]100%|██████████| 318/318 [00:15<00:00, 21.19it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.0123, 'train_samples_per_second': 168.662, 'train_steps_per_second': 21.183, 'train_loss': 0.6393769941989731, 'epoch': 3.0}
Evaluation
2023-05-24 09:40:32.652 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:40:32.652 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:40:32.652 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/partizan/dev.tsv
2023-05-24 09:40:32.655 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:40:32.656 | INFO     | __main__:<module>:108 - 0.45 0.77 1.00 0.53 1.00 0.56
Evaluation
2023-05-24 09:40:33.214 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:40:33.214 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:40:33.215 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/partizan/test.tsv
2023-05-24 09:40:33.218 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:40:33.218 | INFO     | __main__:<module>:108 - 0.46 0.74 0.99 0.56 0.99 0.58
önyargılı
oversampling
Testing turkish , augmentation: önyargılı
2023-05-24 09:40:34.594689: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:40:34.632500: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:40:35.191625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:40:35.695 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:40:35.696 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:35,  1.47it/s]  1%|▏         | 4/318 [00:00<00:52,  6.01it/s]  2%|▏         | 7/318 [00:00<00:31,  9.84it/s]  3%|▎         | 10/318 [00:01<00:23, 12.95it/s]  4%|▍         | 13/318 [00:01<00:19, 15.38it/s]  5%|▌         | 16/318 [00:01<00:17, 17.22it/s]  6%|▌         | 19/318 [00:01<00:16, 18.58it/s]  7%|▋         | 22/318 [00:01<00:15, 19.57it/s]  8%|▊         | 25/318 [00:01<00:14, 20.29it/s]  9%|▉         | 28/318 [00:01<00:13, 20.80it/s] 10%|▉         | 31/318 [00:02<00:13, 21.17it/s] 11%|█         | 34/318 [00:02<00:13, 21.42it/s] 12%|█▏        | 37/318 [00:02<00:13, 21.60it/s] 13%|█▎        | 40/318 [00:02<00:12, 21.72it/s] 14%|█▎        | 43/318 [00:02<00:12, 21.81it/s] 14%|█▍        | 46/318 [00:02<00:12, 21.87it/s] 15%|█▌        | 49/318 [00:02<00:12, 21.92it/s] 16%|█▋        | 52/318 [00:03<00:12, 21.94it/s] 17%|█▋        | 55/318 [00:03<00:11, 21.95it/s] 18%|█▊        | 58/318 [00:03<00:11, 21.91it/s] 19%|█▉        | 61/318 [00:03<00:11, 21.93it/s] 20%|██        | 64/318 [00:03<00:11, 21.94it/s] 21%|██        | 67/318 [00:03<00:11, 21.94it/s] 22%|██▏       | 70/318 [00:03<00:11, 21.96it/s] 23%|██▎       | 73/318 [00:03<00:11, 21.97it/s] 24%|██▍       | 76/318 [00:04<00:11, 21.98it/s] 25%|██▍       | 79/318 [00:04<00:10, 21.98it/s] 26%|██▌       | 82/318 [00:04<00:10, 21.99it/s] 27%|██▋       | 85/318 [00:04<00:10, 21.99it/s] 28%|██▊       | 88/318 [00:04<00:10, 22.00it/s] 29%|██▊       | 91/318 [00:04<00:10, 21.97it/s] 30%|██▉       | 94/318 [00:04<00:10, 21.98it/s] 31%|███       | 97/318 [00:05<00:10, 21.99it/s] 31%|███▏      | 100/318 [00:05<00:09, 21.99it/s] 32%|███▏      | 103/318 [00:05<00:09, 21.98it/s] 33%|███▎      | 106/318 [00:05<00:09, 22.10it/s] 34%|███▍      | 109/318 [00:05<00:09, 22.06it/s] 35%|███▌      | 112/318 [00:05<00:09, 22.04it/s] 36%|███▌      | 115/318 [00:05<00:09, 22.03it/s] 37%|███▋      | 118/318 [00:06<00:09, 22.02it/s] 38%|███▊      | 121/318 [00:06<00:08, 22.02it/s] 39%|███▉      | 124/318 [00:06<00:08, 22.01it/s] 40%|███▉      | 127/318 [00:06<00:08, 22.01it/s] 41%|████      | 130/318 [00:06<00:08, 22.00it/s] 42%|████▏     | 133/318 [00:06<00:08, 22.00it/s] 43%|████▎     | 136/318 [00:06<00:08, 22.00it/s] 44%|████▎     | 139/318 [00:06<00:08, 21.96it/s] 45%|████▍     | 142/318 [00:07<00:08, 21.97it/s] 46%|████▌     | 145/318 [00:07<00:07, 21.97it/s] 47%|████▋     | 148/318 [00:07<00:07, 21.98it/s] 47%|████▋     | 151/318 [00:07<00:07, 21.98it/s] 48%|████▊     | 154/318 [00:07<00:07, 21.99it/s] 49%|████▉     | 157/318 [00:07<00:07, 21.96it/s] 50%|█████     | 160/318 [00:07<00:07, 21.97it/s] 51%|█████▏    | 163/318 [00:08<00:07, 21.98it/s] 52%|█████▏    | 166/318 [00:08<00:06, 21.98it/s] 53%|█████▎    | 169/318 [00:08<00:06, 21.98it/s] 54%|█████▍    | 172/318 [00:08<00:06, 21.99it/s] 55%|█████▌    | 175/318 [00:08<00:06, 21.99it/s] 56%|█████▌    | 178/318 [00:08<00:06, 21.99it/s] 57%|█████▋    | 181/318 [00:08<00:06, 21.99it/s] 58%|█████▊    | 184/318 [00:09<00:06, 21.99it/s] 59%|█████▉    | 187/318 [00:09<00:05, 21.99it/s] 60%|█████▉    | 190/318 [00:09<00:05, 21.96it/s] 61%|██████    | 193/318 [00:09<00:05, 21.97it/s] 62%|██████▏   | 196/318 [00:09<00:05, 21.98it/s] 63%|██████▎   | 199/318 [00:09<00:05, 21.98it/s] 64%|██████▎   | 202/318 [00:09<00:05, 21.99it/s] 64%|██████▍   | 205/318 [00:09<00:05, 21.99it/s] 65%|██████▌   | 208/318 [00:10<00:05, 21.95it/s] 66%|██████▋   | 211/318 [00:10<00:04, 21.96it/s] 67%|██████▋   | 214/318 [00:10<00:04, 22.10it/s] 68%|██████▊   | 217/318 [00:10<00:04, 22.06it/s] 69%|██████▉   | 220/318 [00:10<00:04, 22.04it/s] 70%|███████   | 223/318 [00:10<00:04, 22.03it/s] 71%|███████   | 226/318 [00:10<00:04, 22.02it/s] 72%|███████▏  | 229/318 [00:11<00:04, 22.01it/s] 73%|███████▎  | 232/318 [00:11<00:03, 22.01it/s] 74%|███████▍  | 235/318 [00:11<00:03, 22.01it/s] 75%|███████▍  | 238/318 [00:11<00:03, 21.99it/s] 76%|███████▌  | 241/318 [00:11<00:03, 21.95it/s] 77%|███████▋  | 244/318 [00:11<00:03, 21.97it/s] 78%|███████▊  | 247/318 [00:11<00:03, 21.97it/s] 79%|███████▊  | 250/318 [00:12<00:03, 21.97it/s] 80%|███████▉  | 253/318 [00:12<00:02, 21.97it/s] 81%|████████  | 256/318 [00:12<00:02, 21.94it/s] 81%|████████▏ | 259/318 [00:12<00:02, 21.96it/s] 82%|████████▏ | 262/318 [00:12<00:02, 21.97it/s] 83%|████████▎ | 265/318 [00:12<00:02, 21.98it/s] 84%|████████▍ | 268/318 [00:12<00:02, 21.98it/s] 85%|████████▌ | 271/318 [00:12<00:02, 21.99it/s] 86%|████████▌ | 274/318 [00:13<00:02, 21.99it/s] 87%|████████▋ | 277/318 [00:13<00:01, 21.99it/s] 88%|████████▊ | 280/318 [00:13<00:01, 21.99it/s] 89%|████████▉ | 283/318 [00:13<00:01, 22.00it/s] 90%|████████▉ | 286/318 [00:13<00:01, 22.00it/s] 91%|█████████ | 289/318 [00:13<00:01, 21.99it/s] 92%|█████████▏| 292/318 [00:13<00:01, 21.99it/s] 93%|█████████▎| 295/318 [00:14<00:01, 22.00it/s] 94%|█████████▎| 298/318 [00:14<00:00, 22.00it/s] 95%|█████████▍| 301/318 [00:14<00:00, 22.00it/s] 96%|█████████▌| 304/318 [00:14<00:00, 22.00it/s] 97%|█████████▋| 307/318 [00:14<00:00, 22.00it/s] 97%|█████████▋| 310/318 [00:14<00:00, 22.00it/s] 98%|█████████▊| 313/318 [00:14<00:00, 22.00it/s] 99%|█████████▉| 316/318 [00:15<00:00, 22.00it/s]                                                 100%|██████████| 318/318 [00:15<00:00, 22.00it/s]100%|██████████| 318/318 [00:15<00:00, 21.06it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 15.1038, 'train_samples_per_second': 167.64, 'train_steps_per_second': 21.054, 'train_loss': 0.6411848728011988, 'epoch': 3.0}
Evaluation
2023-05-24 09:41:15.478 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:41:15.478 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:41:15.478 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/önyargılı/dev.tsv
2023-05-24 09:41:15.482 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:41:15.482 | INFO     | __main__:<module>:108 - 0.81 0.84 0.95 0.75 0.95 0.81
Evaluation
2023-05-24 09:41:16.069 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:41:16.069 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:41:16.069 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/önyargılı/test.tsv
2023-05-24 09:41:16.073 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:41:16.073 | INFO     | __main__:<module>:108 - 0.77 0.80 0.91 0.74 0.91 0.78
normal
undersampling
Testing turkish , augmentation: normal
2023-05-24 09:41:17.505799: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:41:17.544159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:41:18.126155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:41:18.631 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7ff16f2de760>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/294 [00:00<?, ?it/s]  0%|          | 1/294 [00:00<03:19,  1.47it/s]  1%|▏         | 4/294 [00:00<00:48,  6.00it/s]  2%|▏         | 7/294 [00:00<00:29,  9.86it/s]  3%|▎         | 10/294 [00:01<00:21, 12.97it/s]  4%|▍         | 13/294 [00:01<00:18, 15.40it/s]  5%|▌         | 16/294 [00:01<00:16, 17.25it/s]  6%|▋         | 19/294 [00:01<00:14, 18.61it/s]  7%|▋         | 22/294 [00:01<00:13, 19.61it/s]  9%|▊         | 25/294 [00:01<00:13, 20.34it/s] 10%|▉         | 28/294 [00:01<00:12, 20.85it/s] 11%|█         | 31/294 [00:02<00:12, 21.22it/s] 12%|█▏        | 34/294 [00:02<00:12, 21.47it/s] 13%|█▎        | 37/294 [00:02<00:11, 21.65it/s] 14%|█▎        | 40/294 [00:02<00:11, 21.78it/s] 15%|█▍        | 43/294 [00:02<00:11, 21.87it/s] 16%|█▌        | 46/294 [00:02<00:11, 21.93it/s] 17%|█▋        | 49/294 [00:02<00:11, 21.97it/s] 18%|█▊        | 52/294 [00:02<00:10, 22.01it/s] 19%|█▊        | 55/294 [00:03<00:10, 22.03it/s] 20%|█▉        | 58/294 [00:03<00:10, 22.05it/s] 21%|██        | 61/294 [00:03<00:10, 22.06it/s] 22%|██▏       | 64/294 [00:03<00:10, 22.05it/s] 23%|██▎       | 67/294 [00:03<00:10, 22.06it/s] 24%|██▍       | 70/294 [00:03<00:10, 22.07it/s] 25%|██▍       | 73/294 [00:03<00:10, 22.07it/s] 26%|██▌       | 76/294 [00:04<00:09, 22.07it/s] 27%|██▋       | 79/294 [00:04<00:09, 22.08it/s] 28%|██▊       | 82/294 [00:04<00:09, 22.08it/s] 29%|██▉       | 85/294 [00:04<00:09, 22.08it/s] 30%|██▉       | 88/294 [00:04<00:09, 22.08it/s] 31%|███       | 91/294 [00:04<00:09, 22.08it/s] 32%|███▏      | 94/294 [00:04<00:09, 22.09it/s] 33%|███▎      | 97/294 [00:05<00:08, 22.00it/s] 34%|███▍      | 100/294 [00:05<00:08, 21.84it/s] 35%|███▌      | 103/294 [00:05<00:08, 21.91it/s] 36%|███▌      | 106/294 [00:05<00:08, 21.97it/s] 37%|███▋      | 109/294 [00:05<00:08, 22.01it/s] 38%|███▊      | 112/294 [00:05<00:08, 22.00it/s] 39%|███▉      | 115/294 [00:05<00:08, 22.03it/s] 40%|████      | 118/294 [00:05<00:07, 22.05it/s] 41%|████      | 121/294 [00:06<00:07, 22.06it/s] 42%|████▏     | 124/294 [00:06<00:07, 22.07it/s] 43%|████▎     | 127/294 [00:06<00:07, 22.07it/s] 44%|████▍     | 130/294 [00:06<00:07, 22.08it/s] 45%|████▌     | 133/294 [00:06<00:07, 22.08it/s] 46%|████▋     | 136/294 [00:06<00:07, 22.09it/s] 47%|████▋     | 139/294 [00:06<00:07, 22.09it/s] 48%|████▊     | 142/294 [00:07<00:06, 22.09it/s] 49%|████▉     | 145/294 [00:07<00:06, 22.09it/s] 50%|█████     | 148/294 [00:07<00:06, 22.09it/s] 51%|█████▏    | 151/294 [00:07<00:06, 22.09it/s] 52%|█████▏    | 154/294 [00:07<00:06, 22.09it/s] 53%|█████▎    | 157/294 [00:07<00:06, 22.10it/s] 54%|█████▍    | 160/294 [00:07<00:06, 22.09it/s] 55%|█████▌    | 163/294 [00:08<00:05, 22.09it/s] 56%|█████▋    | 166/294 [00:08<00:05, 22.09it/s] 57%|█████▋    | 169/294 [00:08<00:05, 22.09it/s] 59%|█████▊    | 172/294 [00:08<00:05, 22.09it/s] 60%|█████▉    | 175/294 [00:08<00:05, 22.09it/s] 61%|██████    | 178/294 [00:08<00:05, 22.09it/s] 62%|██████▏   | 181/294 [00:08<00:05, 22.05it/s] 63%|██████▎   | 184/294 [00:08<00:04, 22.07it/s] 64%|██████▎   | 187/294 [00:09<00:04, 22.07it/s] 65%|██████▍   | 190/294 [00:09<00:04, 22.08it/s] 66%|██████▌   | 193/294 [00:09<00:04, 22.08it/s] 67%|██████▋   | 196/294 [00:09<00:04, 22.20it/s] 68%|██████▊   | 199/294 [00:09<00:04, 22.17it/s] 69%|██████▊   | 202/294 [00:09<00:04, 22.15it/s] 70%|██████▉   | 205/294 [00:09<00:04, 22.13it/s] 71%|███████   | 208/294 [00:10<00:03, 22.10it/s] 72%|███████▏  | 211/294 [00:10<00:03, 22.08it/s] 73%|███████▎  | 214/294 [00:10<00:03, 22.07it/s] 74%|███████▍  | 217/294 [00:10<00:03, 22.06it/s] 75%|███████▍  | 220/294 [00:10<00:03, 22.06it/s] 76%|███████▌  | 223/294 [00:10<00:03, 22.06it/s] 77%|███████▋  | 226/294 [00:10<00:03, 22.05it/s] 78%|███████▊  | 229/294 [00:11<00:02, 22.00it/s] 79%|███████▉  | 232/294 [00:11<00:02, 22.01it/s] 80%|███████▉  | 235/294 [00:11<00:02, 22.02it/s] 81%|████████  | 238/294 [00:11<00:02, 22.03it/s] 82%|████████▏ | 241/294 [00:11<00:02, 22.03it/s] 83%|████████▎ | 244/294 [00:11<00:02, 22.04it/s] 84%|████████▍ | 247/294 [00:11<00:02, 22.04it/s] 85%|████████▌ | 250/294 [00:11<00:01, 22.04it/s] 86%|████████▌ | 253/294 [00:12<00:01, 22.05it/s] 87%|████████▋ | 256/294 [00:12<00:01, 22.06it/s] 88%|████████▊ | 259/294 [00:12<00:01, 22.07it/s] 89%|████████▉ | 262/294 [00:12<00:01, 22.03it/s] 90%|█████████ | 265/294 [00:12<00:01, 22.03it/s] 91%|█████████ | 268/294 [00:12<00:01, 22.04it/s] 92%|█████████▏| 271/294 [00:12<00:01, 22.04it/s] 93%|█████████▎| 274/294 [00:13<00:00, 22.05it/s] 94%|█████████▍| 277/294 [00:13<00:00, 22.05it/s] 95%|█████████▌| 280/294 [00:13<00:00, 22.02it/s] 96%|█████████▋| 283/294 [00:13<00:00, 22.03it/s] 97%|█████████▋| 286/294 [00:13<00:00, 22.04it/s] 98%|█████████▊| 289/294 [00:13<00:00, 22.04it/s] 99%|█████████▉| 292/294 [00:13<00:00, 22.04it/s]                                                 100%|██████████| 294/294 [00:13<00:00, 22.04it/s]100%|██████████| 294/294 [00:13<00:00, 21.05it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 13.9726, 'train_samples_per_second': 167.041, 'train_steps_per_second': 21.041, 'train_loss': 0.6277767363048735, 'epoch': 3.0}
Evaluation
2023-05-24 09:41:56.708 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:41:56.708 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:41:56.708 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/normal/dev.tsv
2023-05-24 09:41:56.712 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:41:56.712 | INFO     | __main__:<module>:108 - 0.79 0.84 0.63 0.95 0.63 0.80
Evaluation
2023-05-24 09:41:57.288 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:41:57.288 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:41:57.289 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/normal/test.tsv
2023-05-24 09:41:57.292 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:41:57.292 | INFO     | __main__:<module>:108 - 0.75 0.80 0.59 0.93 0.59 0.75
duygusal
undersampling
Testing turkish , augmentation: duygusal
2023-05-24 09:41:58.722635: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:41:58.760963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:41:59.344943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:41:59.863 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f0002d9c700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:24,  1.46it/s]  1%|▏         | 4/300 [00:00<00:49,  6.00it/s]  2%|▏         | 7/300 [00:00<00:29,  9.84it/s]  3%|▎         | 10/300 [00:01<00:22, 12.98it/s]  4%|▍         | 13/300 [00:01<00:18, 15.43it/s]  5%|▌         | 16/300 [00:01<00:16, 17.29it/s]  6%|▋         | 19/300 [00:01<00:15, 18.67it/s]  7%|▋         | 22/300 [00:01<00:14, 19.68it/s]  8%|▊         | 25/300 [00:01<00:13, 20.41it/s]  9%|▉         | 28/300 [00:01<00:12, 20.93it/s] 10%|█         | 31/300 [00:02<00:12, 21.30it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.56it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.73it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.87it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.95it/s] 15%|█▌        | 46/300 [00:02<00:11, 22.01it/s] 16%|█▋        | 49/300 [00:02<00:11, 22.06it/s] 17%|█▋        | 52/300 [00:02<00:11, 22.09it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.11it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.13it/s] 20%|██        | 61/300 [00:03<00:10, 22.14it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.14it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.15it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.15it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.15it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.15it/s] 26%|██▋       | 79/300 [00:04<00:09, 22.15it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.16it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.16it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.16it/s] 30%|███       | 91/300 [00:04<00:09, 22.11it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.13it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.14it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.14it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.15it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.04it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.08it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.10it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.12it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.14it/s] 40%|████      | 121/300 [00:06<00:08, 22.10it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.12it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.13it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.14it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.15it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.15it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.16it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.16it/s] 48%|████▊     | 145/300 [00:07<00:06, 22.16it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.15it/s] 50%|█████     | 151/300 [00:07<00:06, 22.14it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.14it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.14it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.13it/s] 54%|█████▍    | 163/300 [00:08<00:06, 22.13it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.13it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.13it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.13it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.10it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.12it/s] 60%|██████    | 181/300 [00:08<00:05, 22.13it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.14it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.15it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.16it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.16it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.16it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.16it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.16it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.12it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.12it/s] 70%|███████   | 211/300 [00:10<00:04, 22.14it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.15it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.15it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.16it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.15it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.15it/s] 76%|███████▋  | 229/300 [00:10<00:03, 22.14it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.15it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.15it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.16it/s] 80%|████████  | 241/300 [00:11<00:02, 22.16it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.16it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.16it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.16it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.16it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.17it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.17it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.16it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.15it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.14it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.13it/s] 91%|█████████▏| 274/300 [00:13<00:01, 22.13it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.13it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.13it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.12it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.08it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.09it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.10it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.11it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.12it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.12it/s]100%|██████████| 300/300 [00:14<00:00, 21.14it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.2001, 'train_samples_per_second': 169.013, 'train_steps_per_second': 21.127, 'train_loss': 0.6345506286621094, 'epoch': 3.0}
Evaluation
2023-05-24 09:42:37.934 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:42:37.935 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:42:37.935 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/duygusal/dev.tsv
2023-05-24 09:42:37.938 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:42:37.938 | INFO     | __main__:<module>:108 - 0.85 0.85 0.84 0.86 0.84 0.85
Evaluation
2023-05-24 09:42:38.521 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:42:38.522 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:42:38.522 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/duygusal/test.tsv
2023-05-24 09:42:38.526 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:42:38.526 | INFO     | __main__:<module>:108 - 0.83 0.83 0.81 0.87 0.81 0.83
propaganda
undersampling
Testing turkish , augmentation: propaganda
2023-05-24 09:42:39.958732: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:42:39.996980: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:42:40.579793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:42:41.097 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fc781242700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:30,  1.42it/s]  1%|▏         | 4/300 [00:00<00:50,  5.85it/s]  2%|▏         | 7/300 [00:00<00:30,  9.67it/s]  3%|▎         | 10/300 [00:01<00:22, 12.80it/s]  4%|▍         | 13/300 [00:01<00:18, 15.27it/s]  5%|▌         | 16/300 [00:01<00:16, 17.16it/s]  6%|▋         | 19/300 [00:01<00:15, 18.56it/s]  7%|▋         | 22/300 [00:01<00:14, 19.59it/s]  8%|▊         | 25/300 [00:01<00:13, 20.33it/s]  9%|▉         | 28/300 [00:01<00:13, 20.87it/s] 10%|█         | 31/300 [00:02<00:12, 21.25it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.52it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.71it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.84it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.93it/s] 15%|█▌        | 46/300 [00:02<00:11, 22.00it/s] 16%|█▋        | 49/300 [00:02<00:11, 22.04it/s] 17%|█▋        | 52/300 [00:03<00:11, 22.07it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.05it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.08it/s] 20%|██        | 61/300 [00:03<00:10, 22.10it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.12it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.12it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.13it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.13it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.13it/s] 26%|██▋       | 79/300 [00:04<00:09, 22.13it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.13it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.13it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.14it/s] 30%|███       | 91/300 [00:04<00:09, 22.14it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.14it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.15it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.15it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.15it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.15it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.15it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.14it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.14it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.15it/s] 40%|████      | 121/300 [00:06<00:08, 22.11it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.12it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.13it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.14it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.14it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.15it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.15it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.14it/s] 48%|████▊     | 145/300 [00:07<00:06, 22.15it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.15it/s] 50%|█████     | 151/300 [00:07<00:06, 22.15it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.13it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.13it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.14it/s] 54%|█████▍    | 163/300 [00:08<00:06, 22.14it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.14it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.15it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.12it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.13it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.13it/s] 60%|██████    | 181/300 [00:08<00:05, 22.14it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.15it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.15it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.15it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.15it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.15it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.15it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.15it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.14it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.15it/s] 70%|███████   | 211/300 [00:10<00:04, 22.15it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.15it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.16it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.15it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.15it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.14it/s] 76%|███████▋  | 229/300 [00:11<00:03, 22.15it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.14it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.14it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.14it/s] 80%|████████  | 241/300 [00:11<00:02, 22.14it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.14it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.14it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.14it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.15it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.15it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.15it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.15it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.14it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.14it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.14it/s] 91%|█████████▏| 274/300 [00:13<00:01, 22.15it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.15it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.15it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.15it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.14it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.13it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.13it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.13it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.14it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.14it/s]100%|██████████| 300/300 [00:14<00:00, 21.10it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.2216, 'train_samples_per_second': 168.757, 'train_steps_per_second': 21.095, 'train_loss': 0.6337547302246094, 'epoch': 3.0}
Evaluation
2023-05-24 09:43:18.618 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:43:18.619 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:43:18.619 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/propaganda/dev.tsv
2023-05-24 09:43:18.622 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:43:18.622 | INFO     | __main__:<module>:108 - 0.85 0.86 0.85 0.86 0.85 0.85
Evaluation
2023-05-24 09:43:19.207 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:43:19.207 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:43:19.207 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/propaganda/test.tsv
2023-05-24 09:43:19.211 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:43:19.211 | INFO     | __main__:<module>:108 - 0.85 0.85 0.81 0.90 0.81 0.85
öznel
undersampling
Testing turkish , augmentation: öznel
2023-05-24 09:43:20.623854: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:43:20.662368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:43:21.240563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:43:21.754 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f800955c700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:23,  1.47it/s]  1%|▏         | 4/300 [00:00<00:49,  6.01it/s]  2%|▏         | 7/300 [00:00<00:29,  9.85it/s]  3%|▎         | 10/300 [00:01<00:22, 12.98it/s]  4%|▍         | 13/300 [00:01<00:18, 15.43it/s]  5%|▌         | 16/300 [00:01<00:16, 17.28it/s]  6%|▋         | 19/300 [00:01<00:15, 18.66it/s]  7%|▋         | 22/300 [00:01<00:14, 19.67it/s]  8%|▊         | 25/300 [00:01<00:13, 20.40it/s]  9%|▉         | 28/300 [00:01<00:13, 20.92it/s] 10%|█         | 31/300 [00:02<00:12, 21.28it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.53it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.73it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.87it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.97it/s] 15%|█▌        | 46/300 [00:02<00:11, 22.04it/s] 16%|█▋        | 49/300 [00:02<00:11, 22.08it/s] 17%|█▋        | 52/300 [00:02<00:11, 22.12it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.14it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.15it/s] 20%|██        | 61/300 [00:03<00:10, 22.16it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.17it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.17it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.18it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.17it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.17it/s] 26%|██▋       | 79/300 [00:04<00:09, 22.17it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.18it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.18it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.18it/s] 30%|███       | 91/300 [00:04<00:09, 22.17it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.17it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.17it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.18it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.18it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.18it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.18it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.18it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.15it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.15it/s] 40%|████      | 121/300 [00:06<00:08, 22.16it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.16it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.17it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.17it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.18it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.18it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.18it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.18it/s] 48%|████▊     | 145/300 [00:07<00:06, 22.18it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.17it/s] 50%|█████     | 151/300 [00:07<00:06, 22.18it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.18it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.18it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.18it/s] 54%|█████▍    | 163/300 [00:07<00:06, 22.18it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.18it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.18it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.18it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.18it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.19it/s] 60%|██████    | 181/300 [00:08<00:05, 22.18it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.18it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.18it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.18it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.18it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.18it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.18it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.18it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.18it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.18it/s] 70%|███████   | 211/300 [00:10<00:04, 22.18it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.18it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.18it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.18it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.18it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.18it/s] 76%|███████▋  | 229/300 [00:10<00:03, 22.17it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.18it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.18it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.18it/s] 80%|████████  | 241/300 [00:11<00:02, 22.17it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.17it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.17it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.17it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.17it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.17it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.17it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.17it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.17it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.17it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.17it/s] 91%|█████████▏| 274/300 [00:12<00:01, 22.17it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.17it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.17it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.17it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.16it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.17it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.17it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.17it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.18it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.18it/s]100%|██████████| 300/300 [00:14<00:00, 21.16it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.1805, 'train_samples_per_second': 169.246, 'train_steps_per_second': 21.156, 'train_loss': 0.6259879557291667, 'epoch': 3.0}
Evaluation
2023-05-24 09:43:59.133 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:43:59.134 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:43:59.134 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/öznel/dev.tsv
2023-05-24 09:43:59.137 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:43:59.137 | INFO     | __main__:<module>:108 - 0.86 0.87 0.92 0.82 0.92 0.86
Evaluation
2023-05-24 09:43:59.724 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:43:59.724 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:43:59.724 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/öznel/test.tsv
2023-05-24 09:43:59.728 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:43:59.728 | INFO     | __main__:<module>:108 - 0.83 0.84 0.89 0.82 0.89 0.83
abartılı
undersampling
Testing turkish , augmentation: abartılı
2023-05-24 09:44:01.135381: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:44:01.172897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:44:01.731859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:44:02.246 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fc7ad5cc700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:20,  1.49it/s]  1%|▏         | 4/300 [00:00<00:48,  6.08it/s]  2%|▏         | 7/300 [00:00<00:29,  9.95it/s]  3%|▎         | 10/300 [00:01<00:22, 13.07it/s]  4%|▍         | 13/300 [00:01<00:18, 15.50it/s]  5%|▌         | 16/300 [00:01<00:16, 17.33it/s]  6%|▋         | 19/300 [00:01<00:15, 18.69it/s]  7%|▋         | 22/300 [00:01<00:14, 19.67it/s]  8%|▊         | 25/300 [00:01<00:13, 20.38it/s]  9%|▉         | 28/300 [00:01<00:13, 20.89it/s] 10%|█         | 31/300 [00:02<00:12, 21.25it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.50it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.68it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.80it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.89it/s] 15%|█▌        | 46/300 [00:02<00:11, 21.94it/s] 16%|█▋        | 49/300 [00:02<00:11, 21.99it/s] 17%|█▋        | 52/300 [00:02<00:11, 22.02it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.03it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.05it/s] 20%|██        | 61/300 [00:03<00:10, 22.06it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.07it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.08it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.07it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.07it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.08it/s] 26%|██▋       | 79/300 [00:04<00:10, 22.08it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.08it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.08it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.08it/s] 30%|███       | 91/300 [00:04<00:09, 22.08it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.08it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.07it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.08it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.08it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.09it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.09it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.09it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.09it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.09it/s] 40%|████      | 121/300 [00:06<00:08, 22.07it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.08it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.08it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.08it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.08it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.09it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.09it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.09it/s] 48%|████▊     | 145/300 [00:07<00:07, 22.09it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.09it/s] 50%|█████     | 151/300 [00:07<00:06, 22.09it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.09it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.08it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.09it/s] 54%|█████▍    | 163/300 [00:08<00:06, 22.09it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.09it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.09it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.09it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.09it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.09it/s] 60%|██████    | 181/300 [00:08<00:05, 22.09it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.09it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.08it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.09it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.09it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.09it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.09it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.09it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.09it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.09it/s] 70%|███████   | 211/300 [00:10<00:04, 22.09it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.09it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.07it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.07it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.03it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.03it/s] 76%|███████▋  | 229/300 [00:10<00:03, 22.03it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.04it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.04it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.05it/s] 80%|████████  | 241/300 [00:11<00:02, 22.07it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.08it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.08it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.08it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.09it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.06it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.07it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.08it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.08it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.09it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.09it/s] 91%|█████████▏| 274/300 [00:13<00:01, 22.06it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.07it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.08it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.09it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.09it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.09it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.10it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.10it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.10it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.10it/s]100%|██████████| 300/300 [00:14<00:00, 21.10it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.2231, 'train_samples_per_second': 168.74, 'train_steps_per_second': 21.092, 'train_loss': 0.6271749369303385, 'epoch': 3.0}
Evaluation
2023-05-24 09:44:40.342 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:44:40.342 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:44:40.342 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/abartılı/dev.tsv
2023-05-24 09:44:40.346 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:44:40.346 | INFO     | __main__:<module>:108 - 0.84 0.85 0.76 0.90 0.76 0.84
Evaluation
2023-05-24 09:44:40.893 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:44:40.894 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:44:40.894 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/abartılı/test.tsv
2023-05-24 09:44:40.897 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:44:40.897 | INFO     | __main__:<module>:108 - 0.80 0.81 0.71 0.90 0.71 0.80
aşağılayıcı
undersampling
Testing turkish , augmentation: aşağılayıcı
2023-05-24 09:44:42.328008: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:44:42.366487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:44:42.947657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:44:43.464 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f09e04d3700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:23,  1.47it/s]  1%|▏         | 4/300 [00:00<00:49,  5.99it/s]  2%|▏         | 7/300 [00:00<00:29,  9.84it/s]  3%|▎         | 10/300 [00:01<00:22, 12.96it/s]  4%|▍         | 13/300 [00:01<00:18, 15.39it/s]  5%|▌         | 16/300 [00:01<00:16, 17.24it/s]  6%|▋         | 19/300 [00:01<00:15, 18.60it/s]  7%|▋         | 22/300 [00:01<00:14, 19.60it/s]  8%|▊         | 25/300 [00:01<00:13, 20.32it/s]  9%|▉         | 28/300 [00:01<00:13, 20.83it/s] 10%|█         | 31/300 [00:02<00:12, 21.20it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.45it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.59it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.73it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.83it/s] 15%|█▌        | 46/300 [00:02<00:11, 21.90it/s] 16%|█▋        | 49/300 [00:02<00:11, 21.95it/s] 17%|█▋        | 52/300 [00:02<00:11, 21.98it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.00it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.02it/s] 20%|██        | 61/300 [00:03<00:10, 22.03it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.04it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.04it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.05it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.05it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.05it/s] 26%|██▋       | 79/300 [00:04<00:10, 22.05it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.05it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.05it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.05it/s] 30%|███       | 91/300 [00:04<00:09, 22.05it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.05it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.05it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.05it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.02it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.03it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.03it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.03it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.04it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.04it/s] 40%|████      | 121/300 [00:06<00:08, 22.04it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.04it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.04it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.04it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.05it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.05it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.05it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.05it/s] 48%|████▊     | 145/300 [00:07<00:07, 22.05it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.05it/s] 50%|█████     | 151/300 [00:07<00:06, 22.02it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.02it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.03it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.04it/s] 54%|█████▍    | 163/300 [00:08<00:06, 22.04it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.04it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.05it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.05it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.05it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.05it/s] 60%|██████    | 181/300 [00:08<00:05, 22.05it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.01it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.02it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.03it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.03it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.03it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.04it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.04it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.04it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.04it/s] 70%|███████   | 211/300 [00:10<00:04, 22.05it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.05it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.05it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.05it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.05it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.05it/s] 76%|███████▋  | 229/300 [00:11<00:03, 22.05it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.05it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.05it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.05it/s] 80%|████████  | 241/300 [00:11<00:02, 22.05it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.05it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.05it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.04it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.04it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.04it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.05it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.05it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.05it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.05it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.06it/s] 91%|█████████▏| 274/300 [00:13<00:01, 22.06it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.06it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.06it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.06it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.05it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.05it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.05it/s] 98%|█████████▊| 295/300 [00:14<00:00, 22.06it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.06it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.06it/s]100%|██████████| 300/300 [00:14<00:00, 21.05it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.2583, 'train_samples_per_second': 168.323, 'train_steps_per_second': 21.04, 'train_loss': 0.6437037150065105, 'epoch': 3.0}
Evaluation
2023-05-24 09:45:20.954 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:45:20.955 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:45:20.955 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/aşağılayıcı/dev.tsv
2023-05-24 09:45:20.958 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:45:20.958 | INFO     | __main__:<module>:108 - 0.86 0.87 0.87 0.86 0.87 0.86
Evaluation
2023-05-24 09:45:21.539 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:45:21.539 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:45:21.539 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/aşağılayıcı/test.tsv
2023-05-24 09:45:21.543 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:45:21.543 | INFO     | __main__:<module>:108 - 0.83 0.83 0.84 0.84 0.84 0.83
partizan
undersampling
Testing turkish , augmentation: partizan
2023-05-24 09:45:22.968950: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:45:23.007174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:45:23.588701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:45:24.106 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb7dfdbe700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:19,  1.50it/s]  1%|▏         | 4/300 [00:00<00:48,  6.10it/s]  2%|▏         | 7/300 [00:00<00:29,  9.99it/s]  3%|▎         | 10/300 [00:01<00:22, 13.13it/s]  4%|▍         | 13/300 [00:01<00:18, 15.56it/s]  5%|▌         | 16/300 [00:01<00:16, 17.41it/s]  6%|▋         | 19/300 [00:01<00:15, 18.73it/s]  7%|▋         | 22/300 [00:01<00:14, 19.73it/s]  8%|▊         | 25/300 [00:01<00:13, 20.45it/s]  9%|▉         | 28/300 [00:01<00:13, 20.92it/s] 10%|█         | 31/300 [00:02<00:12, 21.26it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.50it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.61it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.75it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.85it/s] 15%|█▌        | 46/300 [00:02<00:11, 21.91it/s] 16%|█▋        | 49/300 [00:02<00:11, 21.95it/s] 17%|█▋        | 52/300 [00:02<00:11, 21.98it/s] 18%|█▊        | 55/300 [00:03<00:11, 21.97it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.02it/s] 20%|██        | 61/300 [00:03<00:10, 22.04it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.07it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.08it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.09it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.08it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.09it/s] 26%|██▋       | 79/300 [00:04<00:10, 22.09it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.10it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.10it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.10it/s] 30%|███       | 91/300 [00:04<00:09, 22.10it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.10it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.10it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.10it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.04it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.06it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.07it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.08it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.08it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.09it/s] 40%|████      | 121/300 [00:06<00:08, 22.09it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.09it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.09it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.09it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.10it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.10it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.10it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.10it/s] 48%|████▊     | 145/300 [00:07<00:07, 22.10it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.10it/s] 50%|█████     | 151/300 [00:07<00:06, 22.09it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.09it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.10it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.10it/s] 54%|█████▍    | 163/300 [00:08<00:06, 22.10it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.10it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.10it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.10it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.10it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.10it/s] 60%|██████    | 181/300 [00:08<00:05, 22.10it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.10it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.06it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.07it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.08it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.09it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.09it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.09it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.10it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.08it/s] 70%|███████   | 211/300 [00:10<00:04, 22.09it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.09it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.10it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.10it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.10it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.10it/s] 76%|███████▋  | 229/300 [00:10<00:03, 22.10it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.10it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.06it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.11it/s] 80%|████████  | 241/300 [00:11<00:02, 22.11it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.11it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.10it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.10it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.06it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.08it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.09it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.10it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.10it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.10it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.07it/s] 91%|█████████▏| 274/300 [00:13<00:01, 22.08it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.08it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.09it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.10it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.10it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.11it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.11it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.11it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.10it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.10it/s]100%|██████████| 300/300 [00:14<00:00, 21.11it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.2144, 'train_samples_per_second': 168.843, 'train_steps_per_second': 21.105, 'train_loss': 0.6253354390462239, 'epoch': 3.0}
Evaluation
2023-05-24 09:46:02.434 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:46:02.434 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:46:02.434 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/partizan/dev.tsv
2023-05-24 09:46:02.438 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:46:02.438 | INFO     | __main__:<module>:108 - 0.86 0.86 0.86 0.86 0.86 0.86
Evaluation
2023-05-24 09:46:03.025 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:46:03.025 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:46:03.025 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/partizan/test.tsv
2023-05-24 09:46:03.029 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:46:03.029 | INFO     | __main__:<module>:108 - 0.82 0.83 0.86 0.82 0.86 0.82
önyargılı
undersampling
Testing turkish , augmentation: önyargılı
2023-05-24 09:46:04.438014: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:46:04.475743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:46:05.035012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:46:05.549 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb11811f700>
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:20,  1.49it/s]  1%|▏         | 4/300 [00:00<00:48,  6.10it/s]  2%|▏         | 7/300 [00:00<00:29,  9.97it/s]  3%|▎         | 10/300 [00:01<00:22, 13.10it/s]  4%|▍         | 13/300 [00:01<00:18, 15.54it/s]  5%|▌         | 16/300 [00:01<00:16, 17.38it/s]  6%|▋         | 19/300 [00:01<00:14, 18.74it/s]  7%|▋         | 22/300 [00:01<00:14, 19.70it/s]  8%|▊         | 25/300 [00:01<00:13, 20.42it/s]  9%|▉         | 28/300 [00:01<00:12, 20.94it/s] 10%|█         | 31/300 [00:02<00:12, 21.31it/s] 11%|█▏        | 34/300 [00:02<00:12, 21.56it/s] 12%|█▏        | 37/300 [00:02<00:12, 21.75it/s] 13%|█▎        | 40/300 [00:02<00:11, 21.88it/s] 14%|█▍        | 43/300 [00:02<00:11, 21.97it/s] 15%|█▌        | 46/300 [00:02<00:11, 22.03it/s] 16%|█▋        | 49/300 [00:02<00:11, 22.08it/s] 17%|█▋        | 52/300 [00:02<00:11, 22.10it/s] 18%|█▊        | 55/300 [00:03<00:11, 22.11it/s] 19%|█▉        | 58/300 [00:03<00:10, 22.13it/s] 20%|██        | 61/300 [00:03<00:10, 22.15it/s] 21%|██▏       | 64/300 [00:03<00:10, 22.15it/s] 22%|██▏       | 67/300 [00:03<00:10, 22.16it/s] 23%|██▎       | 70/300 [00:03<00:10, 22.16it/s] 24%|██▍       | 73/300 [00:03<00:10, 22.12it/s] 25%|██▌       | 76/300 [00:04<00:10, 22.13it/s] 26%|██▋       | 79/300 [00:04<00:09, 22.14it/s] 27%|██▋       | 82/300 [00:04<00:09, 22.15it/s] 28%|██▊       | 85/300 [00:04<00:09, 22.16it/s] 29%|██▉       | 88/300 [00:04<00:09, 22.16it/s] 30%|███       | 91/300 [00:04<00:09, 22.16it/s] 31%|███▏      | 94/300 [00:04<00:09, 22.16it/s] 32%|███▏      | 97/300 [00:05<00:09, 22.17it/s] 33%|███▎      | 100/300 [00:05<00:09, 22.17it/s] 34%|███▍      | 103/300 [00:05<00:08, 22.17it/s] 35%|███▌      | 106/300 [00:05<00:08, 22.16it/s] 36%|███▋      | 109/300 [00:05<00:08, 22.16it/s] 37%|███▋      | 112/300 [00:05<00:08, 22.16it/s] 38%|███▊      | 115/300 [00:05<00:08, 22.16it/s] 39%|███▉      | 118/300 [00:05<00:08, 22.16it/s] 40%|████      | 121/300 [00:06<00:08, 22.16it/s] 41%|████▏     | 124/300 [00:06<00:07, 22.09it/s] 42%|████▏     | 127/300 [00:06<00:07, 22.12it/s] 43%|████▎     | 130/300 [00:06<00:07, 22.13it/s] 44%|████▍     | 133/300 [00:06<00:07, 22.14it/s] 45%|████▌     | 136/300 [00:06<00:07, 22.15it/s] 46%|████▋     | 139/300 [00:06<00:07, 22.16it/s] 47%|████▋     | 142/300 [00:07<00:07, 22.16it/s] 48%|████▊     | 145/300 [00:07<00:06, 22.17it/s] 49%|████▉     | 148/300 [00:07<00:06, 22.17it/s] 50%|█████     | 151/300 [00:07<00:06, 22.16it/s] 51%|█████▏    | 154/300 [00:07<00:06, 22.17it/s] 52%|█████▏    | 157/300 [00:07<00:06, 22.17it/s] 53%|█████▎    | 160/300 [00:07<00:06, 22.17it/s] 54%|█████▍    | 163/300 [00:07<00:06, 22.17it/s] 55%|█████▌    | 166/300 [00:08<00:06, 22.17it/s] 56%|█████▋    | 169/300 [00:08<00:05, 22.17it/s] 57%|█████▋    | 172/300 [00:08<00:05, 22.17it/s] 58%|█████▊    | 175/300 [00:08<00:05, 22.17it/s] 59%|█████▉    | 178/300 [00:08<00:05, 22.17it/s] 60%|██████    | 181/300 [00:08<00:05, 22.17it/s] 61%|██████▏   | 184/300 [00:08<00:05, 22.17it/s] 62%|██████▏   | 187/300 [00:09<00:05, 22.17it/s] 63%|██████▎   | 190/300 [00:09<00:04, 22.17it/s] 64%|██████▍   | 193/300 [00:09<00:04, 22.17it/s] 65%|██████▌   | 196/300 [00:09<00:04, 22.17it/s] 66%|██████▋   | 199/300 [00:09<00:04, 22.17it/s] 67%|██████▋   | 202/300 [00:09<00:04, 22.16it/s] 68%|██████▊   | 205/300 [00:09<00:04, 22.16it/s] 69%|██████▉   | 208/300 [00:10<00:04, 22.16it/s] 70%|███████   | 211/300 [00:10<00:04, 22.17it/s] 71%|███████▏  | 214/300 [00:10<00:03, 22.17it/s] 72%|███████▏  | 217/300 [00:10<00:03, 22.17it/s] 73%|███████▎  | 220/300 [00:10<00:03, 22.17it/s] 74%|███████▍  | 223/300 [00:10<00:03, 22.17it/s] 75%|███████▌  | 226/300 [00:10<00:03, 22.16it/s] 76%|███████▋  | 229/300 [00:10<00:03, 22.16it/s] 77%|███████▋  | 232/300 [00:11<00:03, 22.17it/s] 78%|███████▊  | 235/300 [00:11<00:02, 22.16it/s] 79%|███████▉  | 238/300 [00:11<00:02, 22.16it/s] 80%|████████  | 241/300 [00:11<00:02, 22.16it/s] 81%|████████▏ | 244/300 [00:11<00:02, 22.16it/s] 82%|████████▏ | 247/300 [00:11<00:02, 22.16it/s] 83%|████████▎ | 250/300 [00:11<00:02, 22.16it/s] 84%|████████▍ | 253/300 [00:12<00:02, 22.17it/s] 85%|████████▌ | 256/300 [00:12<00:01, 22.17it/s] 86%|████████▋ | 259/300 [00:12<00:01, 22.17it/s] 87%|████████▋ | 262/300 [00:12<00:01, 22.17it/s] 88%|████████▊ | 265/300 [00:12<00:01, 22.17it/s] 89%|████████▉ | 268/300 [00:12<00:01, 22.17it/s] 90%|█████████ | 271/300 [00:12<00:01, 22.17it/s] 91%|█████████▏| 274/300 [00:12<00:01, 22.16it/s] 92%|█████████▏| 277/300 [00:13<00:01, 22.17it/s] 93%|█████████▎| 280/300 [00:13<00:00, 22.17it/s] 94%|█████████▍| 283/300 [00:13<00:00, 22.17it/s] 95%|█████████▌| 286/300 [00:13<00:00, 22.17it/s] 96%|█████████▋| 289/300 [00:13<00:00, 22.17it/s] 97%|█████████▋| 292/300 [00:13<00:00, 22.17it/s] 98%|█████████▊| 295/300 [00:13<00:00, 22.18it/s] 99%|█████████▉| 298/300 [00:14<00:00, 22.18it/s]                                                 100%|██████████| 300/300 [00:14<00:00, 22.18it/s]100%|██████████| 300/300 [00:14<00:00, 21.18it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 14.173, 'train_samples_per_second': 169.336, 'train_steps_per_second': 21.167, 'train_loss': 0.6333028666178385, 'epoch': 3.0}
Evaluation
2023-05-24 09:46:42.880 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:46:42.881 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:46:42.881 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/önyargılı/dev.tsv
2023-05-24 09:46:42.884 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:46:42.884 | INFO     | __main__:<module>:108 - 0.86 0.87 0.79 0.92 0.79 0.86
Evaluation
2023-05-24 09:46:43.469 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:46:43.469 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:46:43.469 | INFO     | __main__:<module>:91 - results/xlm-roberta-base/turkish/önyargılı/test.tsv
2023-05-24 09:46:43.473 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:46:43.473 | INFO     | __main__:<module>:108 - 0.79 0.81 0.67 0.91 0.67 0.79
Experiment
xlm-roberta
dbmdz/bert-base-turkish-128k-cased
turkish
Evaluation
2023-05-24 09:46:44.057 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:46:44.058 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:46:44.058 | INFO     | __main__:<module>:91 - results/xlm-roberta/turkish/no_augment/dev.tsv
2023-05-24 09:46:44.061 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:46:44.061 | INFO     | __main__:<module>:108 - 0.88 0.89 0.86 0.91 0.86 0.89
normal
oversampling
Testing turkish , augmentation: normal
2023-05-24 09:46:45.494484: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:46:45.532637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:46:46.113198: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:46:46.618 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:46:46.619 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:31,  1.50it/s]  1%|▏         | 4/318 [00:00<00:49,  6.29it/s]  2%|▏         | 7/318 [00:00<00:29, 10.58it/s]  3%|▎         | 10/318 [00:01<00:21, 14.21it/s]  4%|▍         | 13/318 [00:01<00:17, 17.15it/s]  5%|▌         | 16/318 [00:01<00:15, 19.44it/s]  6%|▌         | 19/318 [00:01<00:14, 21.17it/s]  7%|▋         | 22/318 [00:01<00:13, 22.46it/s]  8%|▊         | 25/318 [00:01<00:12, 23.35it/s]  9%|▉         | 28/318 [00:01<00:12, 24.04it/s] 10%|▉         | 31/318 [00:01<00:11, 24.53it/s] 11%|█         | 34/318 [00:01<00:11, 24.89it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.14it/s] 13%|█▎        | 40/318 [00:02<00:10, 25.31it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.44it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.47it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.54it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.59it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.63it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.66it/s] 19%|█▉        | 61/318 [00:03<00:10, 25.66it/s] 20%|██        | 64/318 [00:03<00:09, 25.61it/s] 21%|██        | 67/318 [00:03<00:09, 25.63it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.64it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.65it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.65it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.66it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.66it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.67it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.69it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.70it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.71it/s] 31%|███       | 97/318 [00:04<00:08, 25.71it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.71it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.72it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.92it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.86it/s] 35%|███▌      | 112/318 [00:04<00:07, 25.82it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.79it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.76it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.74it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.74it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.69it/s] 41%|████      | 130/318 [00:05<00:07, 25.70it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.71it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.72it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.72it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.72it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.67it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.69it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.70it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.71it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.71it/s] 50%|█████     | 160/318 [00:06<00:06, 25.72it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.72it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.73it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.73it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.73it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.73it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.73it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.73it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.73it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.73it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.73it/s] 61%|██████    | 193/318 [00:08<00:04, 25.73it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.73it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.73it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.73it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.73it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.68it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.69it/s] 67%|██████▋   | 214/318 [00:08<00:04, 25.91it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.85it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.81it/s] 70%|███████   | 223/318 [00:09<00:03, 25.79it/s] 71%|███████   | 226/318 [00:09<00:03, 25.77it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.75it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.75it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.75it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.74it/s] 76%|███████▌  | 241/318 [00:10<00:02, 25.72it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.72it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.73it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.73it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.73it/s] 81%|████████  | 256/318 [00:10<00:02, 25.73it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.73it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.73it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.72it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.72it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.72it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.72it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.72it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.73it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.73it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.73it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.67it/s] 92%|█████████▏| 292/318 [00:11<00:01, 25.69it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.70it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.71it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.71it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.72it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.67it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.68it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.68it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.69it/s]                                                 100%|██████████| 318/318 [00:12<00:00, 25.69it/s]100%|██████████| 318/318 [00:12<00:00, 24.47it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 13.0039, 'train_samples_per_second': 194.71, 'train_steps_per_second': 24.454, 'train_loss': 0.5750022744232753, 'epoch': 3.0}
Evaluation
2023-05-24 09:47:19.579 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:47:19.579 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:47:19.580 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/normal/dev.tsv
2023-05-24 09:47:19.583 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:47:19.583 | INFO     | __main__:<module>:108 - 0.90 0.91 0.91 0.90 0.91 0.91
Evaluation
2023-05-24 09:47:20.166 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:47:20.166 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:47:20.166 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/normal/test.tsv
2023-05-24 09:47:20.170 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:47:20.170 | INFO     | __main__:<module>:108 - 0.86 0.87 0.92 0.84 0.92 0.87
duygusal
oversampling
Testing turkish , augmentation: duygusal
2023-05-24 09:47:21.576690: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:47:21.614940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:47:22.190489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:47:22.697 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:47:22.697 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:31,  1.50it/s]  1%|▏         | 4/318 [00:00<00:49,  6.28it/s]  2%|▏         | 7/318 [00:00<00:29, 10.55it/s]  3%|▎         | 10/318 [00:01<00:21, 14.17it/s]  4%|▍         | 13/318 [00:01<00:17, 17.08it/s]  5%|▌         | 16/318 [00:01<00:15, 19.26it/s]  6%|▌         | 19/318 [00:01<00:14, 20.96it/s]  7%|▋         | 22/318 [00:01<00:13, 22.30it/s]  8%|▊         | 25/318 [00:01<00:12, 23.24it/s]  9%|▉         | 28/318 [00:01<00:12, 23.92it/s] 10%|▉         | 31/318 [00:01<00:11, 24.41it/s] 11%|█         | 34/318 [00:01<00:11, 24.75it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.00it/s] 13%|█▎        | 40/318 [00:02<00:11, 25.14it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.27it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.36it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.42it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.47it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.50it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.52it/s] 19%|█▉        | 61/318 [00:03<00:10, 25.49it/s] 20%|██        | 64/318 [00:03<00:09, 25.52it/s] 21%|██        | 67/318 [00:03<00:09, 25.54it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.55it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.56it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.57it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.57it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.56it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.56it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.57it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.58it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.57it/s] 31%|███       | 97/318 [00:04<00:08, 25.57it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.55it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.57it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.77it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.71it/s] 35%|███▌      | 112/318 [00:05<00:08, 25.67it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.65it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.63it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.61it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.60it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.60it/s] 41%|████      | 130/318 [00:05<00:07, 25.59it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.59it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.59it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.59it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.59it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.58it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.59it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.59it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.59it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.59it/s] 50%|█████     | 160/318 [00:06<00:06, 25.59it/s] 51%|█████▏    | 163/318 [00:07<00:06, 25.53it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.54it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.55it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.56it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.57it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.57it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.53it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.54it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.56it/s] 60%|█████▉    | 190/318 [00:08<00:05, 25.57it/s] 61%|██████    | 193/318 [00:08<00:04, 25.57it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.57it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.58it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.54it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.56it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.57it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.57it/s] 67%|██████▋   | 214/318 [00:08<00:04, 25.77it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.71it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.67it/s] 70%|███████   | 223/318 [00:09<00:03, 25.64it/s] 71%|███████   | 226/318 [00:09<00:03, 25.62it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.61it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.60it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.60it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.60it/s] 76%|███████▌  | 241/318 [00:10<00:03, 25.56it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.57it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.57it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.57it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.58it/s] 81%|████████  | 256/318 [00:10<00:02, 25.58it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.59it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.58it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.58it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.58it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.57it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.58it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.58it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.57it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.52it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.54it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.56it/s] 92%|█████████▏| 292/318 [00:12<00:01, 25.57it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.57it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.57it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.57it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.56it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.57it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.57it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.57it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.57it/s]                                                 100%|██████████| 318/318 [00:13<00:00, 25.57it/s]100%|██████████| 318/318 [00:13<00:00, 24.34it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 13.0727, 'train_samples_per_second': 193.686, 'train_steps_per_second': 24.325, 'train_loss': 0.5847029775943396, 'epoch': 3.0}
Evaluation
2023-05-24 09:47:54.573 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:47:54.574 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:47:54.574 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/duygusal/dev.tsv
2023-05-24 09:47:54.577 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:47:54.577 | INFO     | __main__:<module>:108 - 0.88 0.88 0.91 0.86 0.91 0.88
Evaluation
2023-05-24 09:47:55.161 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:47:55.162 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:47:55.162 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/duygusal/test.tsv
2023-05-24 09:47:55.165 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:47:55.165 | INFO     | __main__:<module>:108 - 0.84 0.85 0.90 0.83 0.90 0.85
propaganda
oversampling
Testing turkish , augmentation: propaganda
2023-05-24 09:47:56.586086: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:47:56.623766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:47:57.192940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:47:57.699 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:47:57.700 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:36,  1.47it/s]  1%|▏         | 4/318 [00:00<00:50,  6.18it/s]  2%|▏         | 7/318 [00:00<00:29, 10.45it/s]  3%|▎         | 10/318 [00:01<00:21, 14.09it/s]  4%|▍         | 13/318 [00:01<00:17, 17.05it/s]  5%|▌         | 16/318 [00:01<00:15, 19.37it/s]  6%|▌         | 19/318 [00:01<00:14, 21.14it/s]  7%|▋         | 22/318 [00:01<00:13, 22.46it/s]  8%|▊         | 25/318 [00:01<00:12, 23.42it/s]  9%|▉         | 28/318 [00:01<00:12, 24.12it/s] 10%|▉         | 31/318 [00:01<00:11, 24.62it/s] 11%|█         | 34/318 [00:01<00:11, 24.97it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.23it/s] 13%|█▎        | 40/318 [00:02<00:10, 25.40it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.53it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.62it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.67it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.71it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.74it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.77it/s] 19%|█▉        | 61/318 [00:03<00:09, 25.78it/s] 20%|██        | 64/318 [00:03<00:09, 25.79it/s] 21%|██        | 67/318 [00:03<00:09, 25.80it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.80it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.81it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.81it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.81it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.81it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.82it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.82it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.82it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.82it/s] 31%|███       | 97/318 [00:04<00:08, 25.82it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.82it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.82it/s] 33%|███▎      | 106/318 [00:04<00:08, 26.03it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.97it/s] 35%|███▌      | 112/318 [00:04<00:07, 25.93it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.90it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.88it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.85it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.84it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.84it/s] 41%|████      | 130/318 [00:05<00:07, 25.83it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.83it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.83it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.83it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.82it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.81it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.82it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.82it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.81it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.81it/s] 50%|█████     | 160/318 [00:06<00:06, 25.82it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.82it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.82it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.82it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.82it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.82it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.82it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.81it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.80it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.80it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.81it/s] 61%|██████    | 193/318 [00:08<00:04, 25.81it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.81it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.81it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.81it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.82it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.82it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.83it/s] 67%|██████▋   | 214/318 [00:08<00:03, 26.05it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.98it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.92it/s] 70%|███████   | 223/318 [00:09<00:03, 25.89it/s] 71%|███████   | 226/318 [00:09<00:03, 25.87it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.86it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.85it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.84it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.84it/s] 76%|███████▌  | 241/318 [00:09<00:02, 25.81it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.82it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.81it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.81it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.81it/s] 81%|████████  | 256/318 [00:10<00:02, 25.82it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.82it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.82it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.82it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.82it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.82it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.82it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.82it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.82it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.83it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.82it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.82it/s] 92%|█████████▏| 292/318 [00:11<00:01, 25.82it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.82it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.82it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.82it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.82it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.82it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.81it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.81it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.81it/s]                                                 100%|██████████| 318/318 [00:12<00:00, 25.81it/s]100%|██████████| 318/318 [00:12<00:00, 24.54it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.9662, 'train_samples_per_second': 195.277, 'train_steps_per_second': 24.525, 'train_loss': 0.596212183154604, 'epoch': 3.0}
Evaluation
2023-05-24 09:48:28.974 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:48:28.975 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:48:28.975 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/propaganda/dev.tsv
2023-05-24 09:48:28.978 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:48:28.978 | INFO     | __main__:<module>:108 - 0.86 0.88 0.96 0.81 0.96 0.86
Evaluation
2023-05-24 09:48:29.563 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:48:29.564 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:48:29.564 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/propaganda/test.tsv
2023-05-24 09:48:29.568 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:48:29.568 | INFO     | __main__:<module>:108 - 0.86 0.88 0.95 0.82 0.95 0.86
öznel
oversampling
Testing turkish , augmentation: öznel
2023-05-24 09:48:30.972104: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:48:31.009740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:48:31.570319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:48:32.076 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:48:32.076 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:30,  1.51it/s]  1%|▏         | 4/318 [00:00<00:49,  6.30it/s]  2%|▏         | 7/318 [00:00<00:29, 10.58it/s]  3%|▎         | 10/318 [00:01<00:21, 14.20it/s]  4%|▍         | 13/318 [00:01<00:17, 17.12it/s]  5%|▌         | 16/318 [00:01<00:15, 19.39it/s]  6%|▌         | 19/318 [00:01<00:14, 21.11it/s]  7%|▋         | 22/318 [00:01<00:13, 22.38it/s]  8%|▊         | 25/318 [00:01<00:12, 23.31it/s]  9%|▉         | 28/318 [00:01<00:12, 23.98it/s] 10%|▉         | 31/318 [00:01<00:11, 24.46it/s] 11%|█         | 34/318 [00:01<00:11, 24.80it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.05it/s] 13%|█▎        | 40/318 [00:02<00:11, 25.22it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.34it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.38it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.45it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.50it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.54it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.57it/s] 19%|█▉        | 61/318 [00:03<00:10, 25.58it/s] 20%|██        | 64/318 [00:03<00:09, 25.56it/s] 21%|██        | 67/318 [00:03<00:09, 25.57it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.59it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.60it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.60it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.61it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.61it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.61it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.62it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.63it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.64it/s] 31%|███       | 97/318 [00:04<00:08, 25.64it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.64it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.64it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.85it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.78it/s] 35%|███▌      | 112/318 [00:04<00:08, 25.73it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.70it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.68it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.66it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.65it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.63it/s] 41%|████      | 130/318 [00:05<00:07, 25.62it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.62it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.62it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.62it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.62it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.58it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.60it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.60it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.61it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.62it/s] 50%|█████     | 160/318 [00:06<00:06, 25.63it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.61it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.55it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.55it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.58it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.59it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.60it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.61it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.61it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.62it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.63it/s] 61%|██████    | 193/318 [00:08<00:04, 25.62it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.63it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.62it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.63it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.57it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.58it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.60it/s] 67%|██████▋   | 214/318 [00:08<00:04, 25.82it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.76it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.72it/s] 70%|███████   | 223/318 [00:09<00:03, 25.69it/s] 71%|███████   | 226/318 [00:09<00:03, 25.67it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.66it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.65it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.64it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.63it/s] 76%|███████▌  | 241/318 [00:10<00:03, 25.60it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.62it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.63it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.66it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.67it/s] 81%|████████  | 256/318 [00:10<00:02, 25.69it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.70it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.71it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.70it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.70it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.71it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.71it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.72it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.72it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.72it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.72it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.72it/s] 92%|█████████▏| 292/318 [00:12<00:01, 25.72it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.72it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.72it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.72it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.73it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.72it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.73it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.72it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.72it/s]                                                 100%|██████████| 318/318 [00:13<00:00, 25.72it/s]100%|██████████| 318/318 [00:13<00:00, 24.41it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 13.035, 'train_samples_per_second': 194.246, 'train_steps_per_second': 24.396, 'train_loss': 0.5908991015932095, 'epoch': 3.0}
Evaluation
2023-05-24 09:49:03.569 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:49:03.569 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:49:03.569 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/öznel/dev.tsv
2023-05-24 09:49:03.572 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:49:03.573 | INFO     | __main__:<module>:108 - 0.88 0.89 0.94 0.84 0.94 0.88
Evaluation
2023-05-24 09:49:04.159 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:49:04.160 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:49:04.160 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/öznel/test.tsv
2023-05-24 09:49:04.163 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:49:04.164 | INFO     | __main__:<module>:108 - 0.83 0.84 0.91 0.80 0.91 0.83
abartılı
oversampling
Testing turkish , augmentation: abartılı
2023-05-24 09:49:05.563935: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:49:05.602156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:49:06.183771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:49:06.689 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:49:06.690 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:31,  1.50it/s]  1%|▏         | 4/318 [00:00<00:49,  6.30it/s]  2%|▏         | 7/318 [00:00<00:29, 10.60it/s]  3%|▎         | 10/318 [00:01<00:21, 14.21it/s]  4%|▍         | 13/318 [00:01<00:17, 17.16it/s]  5%|▌         | 16/318 [00:01<00:15, 19.46it/s]  6%|▌         | 19/318 [00:01<00:14, 21.21it/s]  7%|▋         | 22/318 [00:01<00:13, 22.50it/s]  8%|▊         | 25/318 [00:01<00:12, 23.45it/s]  9%|▉         | 28/318 [00:01<00:12, 24.13it/s] 10%|▉         | 31/318 [00:01<00:11, 24.58it/s] 11%|█         | 34/318 [00:01<00:11, 24.94it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.19it/s] 13%|█▎        | 40/318 [00:02<00:10, 25.37it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.49it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.58it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.64it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.68it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.72it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.74it/s] 19%|█▉        | 61/318 [00:02<00:09, 25.75it/s] 20%|██        | 64/318 [00:03<00:09, 25.77it/s] 21%|██        | 67/318 [00:03<00:09, 25.77it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.78it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.75it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.76it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.77it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.76it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.77it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.77it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.78it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.79it/s] 31%|███       | 97/318 [00:04<00:08, 25.78it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.78it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.79it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.98it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.92it/s] 35%|███▌      | 112/318 [00:04<00:07, 25.88it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.86it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.84it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.82it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.82it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.81it/s] 41%|████      | 130/318 [00:05<00:07, 25.80it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.80it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.79it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.79it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.79it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.79it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.79it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.78it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.77it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.74it/s] 50%|█████     | 160/318 [00:06<00:06, 25.75it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.77it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.78it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.78it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.79it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.75it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.76it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.76it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.78it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.78it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.78it/s] 61%|██████    | 193/318 [00:08<00:04, 25.75it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.76it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.77it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.78it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.79it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.80it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.80it/s] 67%|██████▋   | 214/318 [00:08<00:04, 26.00it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.93it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.89it/s] 70%|███████   | 223/318 [00:09<00:03, 25.86it/s] 71%|███████   | 226/318 [00:09<00:03, 25.84it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.83it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.82it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.82it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.81it/s] 76%|███████▌  | 241/318 [00:09<00:02, 25.79it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.77it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.78it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.78it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.77it/s] 81%|████████  | 256/318 [00:10<00:02, 25.77it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.76it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.76it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.75it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.75it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.74it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.74it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.67it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.69it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.72it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.75it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.76it/s] 92%|█████████▏| 292/318 [00:11<00:01, 25.76it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.73it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.76it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.78it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.79it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.79it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.79it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.80it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.80it/s]                                                 100%|██████████| 318/318 [00:12<00:00, 25.80it/s]100%|██████████| 318/318 [00:12<00:00, 24.53it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.9712, 'train_samples_per_second': 195.201, 'train_steps_per_second': 24.516, 'train_loss': 0.5800563884231279, 'epoch': 3.0}
Evaluation
2023-05-24 09:49:37.686 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:49:37.686 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:49:37.686 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/abartılı/dev.tsv
2023-05-24 09:49:37.689 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:49:37.690 | INFO     | __main__:<module>:108 - 0.91 0.91 0.96 0.87 0.96 0.91
Evaluation
2023-05-24 09:49:38.271 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:49:38.272 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:49:38.272 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/abartılı/test.tsv
2023-05-24 09:49:38.276 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:49:38.276 | INFO     | __main__:<module>:108 - 0.86 0.86 0.92 0.83 0.92 0.86
aşağılayıcı
oversampling
Testing turkish , augmentation: aşağılayıcı
2023-05-24 09:49:39.726511: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:49:39.764975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:49:40.347462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:49:40.852 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:49:40.853 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:40,  1.44it/s]  1%|▏         | 4/318 [00:00<00:51,  6.07it/s]  2%|▏         | 7/318 [00:00<00:30, 10.27it/s]  3%|▎         | 10/318 [00:01<00:22, 13.88it/s]  4%|▍         | 13/318 [00:01<00:18, 16.82it/s]  5%|▌         | 16/318 [00:01<00:15, 19.10it/s]  6%|▌         | 19/318 [00:01<00:14, 20.87it/s]  7%|▋         | 22/318 [00:01<00:13, 22.19it/s]  8%|▊         | 25/318 [00:01<00:12, 23.16it/s]  9%|▉         | 28/318 [00:01<00:12, 23.86it/s] 10%|▉         | 31/318 [00:01<00:11, 24.36it/s] 11%|█         | 34/318 [00:01<00:11, 24.72it/s] 12%|█▏        | 37/318 [00:02<00:11, 24.97it/s] 13%|█▎        | 40/318 [00:02<00:11, 25.15it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.27it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.34it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.39it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.43it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.46it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.48it/s] 19%|█▉        | 61/318 [00:03<00:10, 25.49it/s] 20%|██        | 64/318 [00:03<00:09, 25.50it/s] 21%|██        | 67/318 [00:03<00:09, 25.51it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.51it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.51it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.52it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.53it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.49it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.52it/s] 28%|██▊       | 88/318 [00:04<00:09, 25.53it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.55it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.56it/s] 31%|███       | 97/318 [00:04<00:08, 25.56it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.57it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.57it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.75it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.70it/s] 35%|███▌      | 112/318 [00:05<00:08, 25.66it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.64it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.62it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.60it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.60it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.59it/s] 41%|████      | 130/318 [00:05<00:07, 25.58it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.58it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.58it/s] 44%|████▎     | 139/318 [00:06<00:07, 25.57it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.56it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.54it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.53it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.47it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.49it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.50it/s] 50%|█████     | 160/318 [00:06<00:06, 25.50it/s] 51%|█████▏    | 163/318 [00:07<00:06, 25.51it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.51it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.51it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.53it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.53it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.53it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.53it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.54it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.54it/s] 60%|█████▉    | 190/318 [00:08<00:05, 25.54it/s] 61%|██████    | 193/318 [00:08<00:04, 25.54it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.55it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.55it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.55it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.55it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.55it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.55it/s] 67%|██████▋   | 214/318 [00:09<00:04, 25.73it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.67it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.63it/s] 70%|███████   | 223/318 [00:09<00:03, 25.60it/s] 71%|███████   | 226/318 [00:09<00:03, 25.59it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.58it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.57it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.57it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.56it/s] 76%|███████▌  | 241/318 [00:10<00:03, 25.53it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.53it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.54it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.50it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.51it/s] 81%|████████  | 256/318 [00:10<00:02, 25.53it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.53it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.54it/s] 83%|████████▎ | 265/318 [00:11<00:02, 25.53it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.54it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.54it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.55it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.55it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.55it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.51it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.57it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.56it/s] 92%|█████████▏| 292/318 [00:12<00:01, 25.56it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.56it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.55it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.55it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.55it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.56it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.55it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.55it/s] 99%|█████████▉| 316/318 [00:13<00:00, 25.56it/s]                                                 100%|██████████| 318/318 [00:13<00:00, 25.56it/s]100%|██████████| 318/318 [00:13<00:00, 24.26it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 13.1131, 'train_samples_per_second': 193.089, 'train_steps_per_second': 24.251, 'train_loss': 0.6128148492777122, 'epoch': 3.0}
Evaluation
2023-05-24 09:50:12.324 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:50:12.325 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:50:12.325 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/aşağılayıcı/dev.tsv
2023-05-24 09:50:12.328 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:50:12.328 | INFO     | __main__:<module>:108 - 0.88 0.89 0.96 0.83 0.96 0.88
Evaluation
2023-05-24 09:50:12.922 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:50:12.922 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:50:12.922 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/aşağılayıcı/test.tsv
2023-05-24 09:50:12.926 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:50:12.926 | INFO     | __main__:<module>:108 - 0.82 0.84 0.92 0.79 0.92 0.83
partizan
oversampling
Testing turkish , augmentation: partizan
2023-05-24 09:50:14.382893: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:50:14.421673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:50:14.999339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:50:15.502 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:50:15.503 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:33,  1.49it/s]  1%|▏         | 4/318 [00:00<00:50,  6.24it/s]  2%|▏         | 7/318 [00:00<00:29, 10.53it/s]  3%|▎         | 10/318 [00:01<00:21, 14.16it/s]  4%|▍         | 13/318 [00:01<00:17, 17.12it/s]  5%|▌         | 16/318 [00:01<00:15, 19.43it/s]  6%|▌         | 19/318 [00:01<00:14, 21.18it/s]  7%|▋         | 22/318 [00:01<00:13, 22.49it/s]  8%|▊         | 25/318 [00:01<00:12, 23.44it/s]  9%|▉         | 28/318 [00:01<00:12, 24.12it/s] 10%|▉         | 31/318 [00:01<00:11, 24.62it/s] 11%|█         | 34/318 [00:01<00:11, 24.96it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.21it/s] 13%|█▎        | 40/318 [00:02<00:10, 25.39it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.51it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.60it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.65it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.69it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.72it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.74it/s] 19%|█▉        | 61/318 [00:03<00:09, 25.75it/s] 20%|██        | 64/318 [00:03<00:09, 25.76it/s] 21%|██        | 67/318 [00:03<00:09, 25.50it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.58it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.65it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.69it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.72it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.74it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.76it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.77it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.78it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.78it/s] 31%|███       | 97/318 [00:04<00:08, 25.79it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.79it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.79it/s] 33%|███▎      | 106/318 [00:04<00:08, 26.00it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.94it/s] 35%|███▌      | 112/318 [00:04<00:07, 25.91it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.88it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.86it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.83it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.83it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.81it/s] 41%|████      | 130/318 [00:05<00:07, 25.81it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.81it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.77it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.82it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.81it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.81it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.80it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.80it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.80it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.79it/s] 50%|█████     | 160/318 [00:06<00:06, 25.80it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.80it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.81it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.80it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.80it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.80it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.80it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.80it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.80it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.79it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.80it/s] 61%|██████    | 193/318 [00:08<00:04, 25.80it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.80it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.80it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.81it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.75it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.76it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.78it/s] 67%|██████▋   | 214/318 [00:08<00:04, 25.99it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.93it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.88it/s] 70%|███████   | 223/318 [00:09<00:03, 25.84it/s] 71%|███████   | 226/318 [00:09<00:03, 25.82it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.80it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.79it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.79it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.78it/s] 76%|███████▌  | 241/318 [00:09<00:02, 25.77it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.77it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.77it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.76it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.77it/s] 81%|████████  | 256/318 [00:10<00:02, 25.77it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.77it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.78it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.77it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.77it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.77it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.71it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.73it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.75it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.76it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.77it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.77it/s] 92%|█████████▏| 292/318 [00:11<00:01, 25.77it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.77it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.78it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.78it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.78it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.78it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.78it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.77it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.77it/s]                                                 100%|██████████| 318/318 [00:12<00:00, 25.77it/s]100%|██████████| 318/318 [00:12<00:00, 24.52it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.9767, 'train_samples_per_second': 195.119, 'train_steps_per_second': 24.506, 'train_loss': 0.6077852069206957, 'epoch': 3.0}
Evaluation
2023-05-24 09:50:46.412 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:50:46.412 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:50:46.412 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/partizan/dev.tsv
2023-05-24 09:50:46.416 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:50:46.416 | INFO     | __main__:<module>:108 - 0.88 0.88 0.85 0.90 0.85 0.88
Evaluation
2023-05-24 09:50:46.999 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:50:46.999 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:50:46.999 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/partizan/test.tsv
2023-05-24 09:50:47.003 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:50:47.003 | INFO     | __main__:<module>:108 - 0.84 0.85 0.89 0.83 0.89 0.85
önyargılı
oversampling
Testing turkish , augmentation: önyargılı
2023-05-24 09:50:48.415452: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:50:48.453315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:50:49.013537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:50:49.519 | INFO     | app.models.transformer_models:train:81 - Number of the documents: 844
2023-05-24 09:50:49.520 | INFO     | app.models.transformer_models:train:83 - Number of the documents: 844
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/318 [00:00<?, ?it/s]  0%|          | 1/318 [00:00<03:36,  1.47it/s]  1%|▏         | 4/318 [00:00<00:50,  6.17it/s]  2%|▏         | 7/318 [00:00<00:29, 10.43it/s]  3%|▎         | 10/318 [00:01<00:21, 14.06it/s]  4%|▍         | 13/318 [00:01<00:17, 17.02it/s]  5%|▌         | 16/318 [00:01<00:15, 19.34it/s]  6%|▌         | 19/318 [00:01<00:14, 21.10it/s]  7%|▋         | 22/318 [00:01<00:13, 22.42it/s]  8%|▊         | 25/318 [00:01<00:12, 23.38it/s]  9%|▉         | 28/318 [00:01<00:12, 24.07it/s] 10%|▉         | 31/318 [00:01<00:11, 24.57it/s] 11%|█         | 34/318 [00:01<00:11, 24.87it/s] 12%|█▏        | 37/318 [00:02<00:11, 25.14it/s] 13%|█▎        | 40/318 [00:02<00:10, 25.32it/s] 14%|█▎        | 43/318 [00:02<00:10, 25.46it/s] 14%|█▍        | 46/318 [00:02<00:10, 25.55it/s] 15%|█▌        | 49/318 [00:02<00:10, 25.61it/s] 16%|█▋        | 52/318 [00:02<00:10, 25.66it/s] 17%|█▋        | 55/318 [00:02<00:10, 25.69it/s] 18%|█▊        | 58/318 [00:02<00:10, 25.71it/s] 19%|█▉        | 61/318 [00:03<00:09, 25.73it/s] 20%|██        | 64/318 [00:03<00:09, 25.74it/s] 21%|██        | 67/318 [00:03<00:09, 25.75it/s] 22%|██▏       | 70/318 [00:03<00:09, 25.75it/s] 23%|██▎       | 73/318 [00:03<00:09, 25.75it/s] 24%|██▍       | 76/318 [00:03<00:09, 25.76it/s] 25%|██▍       | 79/318 [00:03<00:09, 25.76it/s] 26%|██▌       | 82/318 [00:03<00:09, 25.71it/s] 27%|██▋       | 85/318 [00:03<00:09, 25.72it/s] 28%|██▊       | 88/318 [00:04<00:08, 25.74it/s] 29%|██▊       | 91/318 [00:04<00:08, 25.75it/s] 30%|██▉       | 94/318 [00:04<00:08, 25.75it/s] 31%|███       | 97/318 [00:04<00:08, 25.76it/s] 31%|███▏      | 100/318 [00:04<00:08, 25.76it/s] 32%|███▏      | 103/318 [00:04<00:08, 25.76it/s] 33%|███▎      | 106/318 [00:04<00:08, 25.98it/s] 34%|███▍      | 109/318 [00:04<00:08, 25.91it/s] 35%|███▌      | 112/318 [00:04<00:07, 25.87it/s] 36%|███▌      | 115/318 [00:05<00:07, 25.84it/s] 37%|███▋      | 118/318 [00:05<00:07, 25.82it/s] 38%|███▊      | 121/318 [00:05<00:07, 25.80it/s] 39%|███▉      | 124/318 [00:05<00:07, 25.78it/s] 40%|███▉      | 127/318 [00:05<00:07, 25.78it/s] 41%|████      | 130/318 [00:05<00:07, 25.75it/s] 42%|████▏     | 133/318 [00:05<00:07, 25.78it/s] 43%|████▎     | 136/318 [00:05<00:07, 25.78it/s] 44%|████▎     | 139/318 [00:06<00:06, 25.77it/s] 45%|████▍     | 142/318 [00:06<00:06, 25.77it/s] 46%|████▌     | 145/318 [00:06<00:06, 25.77it/s] 47%|████▋     | 148/318 [00:06<00:06, 25.72it/s] 47%|████▋     | 151/318 [00:06<00:06, 25.73it/s] 48%|████▊     | 154/318 [00:06<00:06, 25.75it/s] 49%|████▉     | 157/318 [00:06<00:06, 25.75it/s] 50%|█████     | 160/318 [00:06<00:06, 25.76it/s] 51%|█████▏    | 163/318 [00:06<00:06, 25.71it/s] 52%|█████▏    | 166/318 [00:07<00:05, 25.73it/s] 53%|█████▎    | 169/318 [00:07<00:05, 25.74it/s] 54%|█████▍    | 172/318 [00:07<00:05, 25.75it/s] 55%|█████▌    | 175/318 [00:07<00:05, 25.75it/s] 56%|█████▌    | 178/318 [00:07<00:05, 25.76it/s] 57%|█████▋    | 181/318 [00:07<00:05, 25.76it/s] 58%|█████▊    | 184/318 [00:07<00:05, 25.76it/s] 59%|█████▉    | 187/318 [00:07<00:05, 25.76it/s] 60%|█████▉    | 190/318 [00:08<00:04, 25.76it/s] 61%|██████    | 193/318 [00:08<00:04, 25.76it/s] 62%|██████▏   | 196/318 [00:08<00:04, 25.77it/s] 63%|██████▎   | 199/318 [00:08<00:04, 25.77it/s] 64%|██████▎   | 202/318 [00:08<00:04, 25.76it/s] 64%|██████▍   | 205/318 [00:08<00:04, 25.77it/s] 65%|██████▌   | 208/318 [00:08<00:04, 25.77it/s] 66%|██████▋   | 211/318 [00:08<00:04, 25.76it/s] 67%|██████▋   | 214/318 [00:08<00:04, 26.00it/s] 68%|██████▊   | 217/318 [00:09<00:03, 25.92it/s] 69%|██████▉   | 220/318 [00:09<00:03, 25.87it/s] 70%|███████   | 223/318 [00:09<00:03, 25.84it/s] 71%|███████   | 226/318 [00:09<00:03, 25.82it/s] 72%|███████▏  | 229/318 [00:09<00:03, 25.76it/s] 73%|███████▎  | 232/318 [00:09<00:03, 25.77it/s] 74%|███████▍  | 235/318 [00:09<00:03, 25.77it/s] 75%|███████▍  | 238/318 [00:09<00:03, 25.77it/s] 76%|███████▌  | 241/318 [00:09<00:02, 25.75it/s] 77%|███████▋  | 244/318 [00:10<00:02, 25.75it/s] 78%|███████▊  | 247/318 [00:10<00:02, 25.75it/s] 79%|███████▊  | 250/318 [00:10<00:02, 25.76it/s] 80%|███████▉  | 253/318 [00:10<00:02, 25.76it/s] 81%|████████  | 256/318 [00:10<00:02, 25.77it/s] 81%|████████▏ | 259/318 [00:10<00:02, 25.77it/s] 82%|████████▏ | 262/318 [00:10<00:02, 25.72it/s] 83%|████████▎ | 265/318 [00:10<00:02, 25.74it/s] 84%|████████▍ | 268/318 [00:11<00:01, 25.74it/s] 85%|████████▌ | 271/318 [00:11<00:01, 25.76it/s] 86%|████████▌ | 274/318 [00:11<00:01, 25.76it/s] 87%|████████▋ | 277/318 [00:11<00:01, 25.77it/s] 88%|████████▊ | 280/318 [00:11<00:01, 25.76it/s] 89%|████████▉ | 283/318 [00:11<00:01, 25.74it/s] 90%|████████▉ | 286/318 [00:11<00:01, 25.73it/s] 91%|█████████ | 289/318 [00:11<00:01, 25.73it/s] 92%|█████████▏| 292/318 [00:11<00:01, 25.67it/s] 93%|█████████▎| 295/318 [00:12<00:00, 25.73it/s] 94%|█████████▎| 298/318 [00:12<00:00, 25.73it/s] 95%|█████████▍| 301/318 [00:12<00:00, 25.73it/s] 96%|█████████▌| 304/318 [00:12<00:00, 25.72it/s] 97%|█████████▋| 307/318 [00:12<00:00, 25.72it/s] 97%|█████████▋| 310/318 [00:12<00:00, 25.66it/s] 98%|█████████▊| 313/318 [00:12<00:00, 25.68it/s] 99%|█████████▉| 316/318 [00:12<00:00, 25.71it/s]                                                 100%|██████████| 318/318 [00:12<00:00, 25.71it/s]100%|██████████| 318/318 [00:12<00:00, 24.47it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.9995, 'train_samples_per_second': 194.777, 'train_steps_per_second': 24.462, 'train_loss': 0.5867136829304245, 'epoch': 3.0}
Evaluation
2023-05-24 09:51:20.776 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:51:20.777 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:51:20.777 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/önyargılı/dev.tsv
2023-05-24 09:51:20.780 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:51:20.780 | INFO     | __main__:<module>:108 - 0.89 0.90 0.95 0.85 0.95 0.89
Evaluation
2023-05-24 09:51:21.363 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:51:21.363 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:51:21.363 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/önyargılı/test.tsv
2023-05-24 09:51:21.367 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:51:21.367 | INFO     | __main__:<module>:108 - 0.86 0.87 0.93 0.83 0.93 0.86
normal
undersampling
Testing turkish , augmentation: normal
2023-05-24 09:51:22.805446: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:51:22.843911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:51:23.420365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:51:23.928 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9dcfa0f760>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/294 [00:00<?, ?it/s]  0%|          | 1/294 [00:00<03:17,  1.48it/s]  1%|▏         | 4/294 [00:00<00:46,  6.23it/s]  2%|▏         | 7/294 [00:00<00:27, 10.50it/s]  3%|▎         | 10/294 [00:01<00:20, 14.12it/s]  4%|▍         | 13/294 [00:01<00:16, 17.02it/s]  5%|▌         | 16/294 [00:01<00:14, 19.32it/s]  6%|▋         | 19/294 [00:01<00:13, 21.06it/s]  7%|▋         | 22/294 [00:01<00:12, 22.36it/s]  9%|▊         | 25/294 [00:01<00:11, 23.31it/s] 10%|▉         | 28/294 [00:01<00:11, 23.99it/s] 11%|█         | 31/294 [00:01<00:10, 24.48it/s] 12%|█▏        | 34/294 [00:01<00:10, 24.83it/s] 13%|█▎        | 37/294 [00:02<00:10, 25.07it/s] 14%|█▎        | 40/294 [00:02<00:10, 25.24it/s] 15%|█▍        | 43/294 [00:02<00:09, 25.36it/s] 16%|█▌        | 46/294 [00:02<00:09, 25.45it/s] 17%|█▋        | 49/294 [00:02<00:09, 25.51it/s] 18%|█▊        | 52/294 [00:02<00:09, 25.56it/s] 19%|█▊        | 55/294 [00:02<00:09, 25.59it/s] 20%|█▉        | 58/294 [00:02<00:09, 25.60it/s] 21%|██        | 61/294 [00:03<00:09, 25.61it/s] 22%|██▏       | 64/294 [00:03<00:08, 25.62it/s] 23%|██▎       | 67/294 [00:03<00:08, 25.63it/s] 24%|██▍       | 70/294 [00:03<00:08, 25.63it/s] 25%|██▍       | 73/294 [00:03<00:08, 25.64it/s] 26%|██▌       | 76/294 [00:03<00:08, 25.63it/s] 27%|██▋       | 79/294 [00:03<00:08, 25.64it/s] 28%|██▊       | 82/294 [00:03<00:08, 25.64it/s] 29%|██▉       | 85/294 [00:03<00:08, 25.64it/s] 30%|██▉       | 88/294 [00:04<00:08, 25.65it/s] 31%|███       | 91/294 [00:04<00:07, 25.65it/s] 32%|███▏      | 94/294 [00:04<00:07, 25.65it/s] 33%|███▎      | 97/294 [00:04<00:07, 25.66it/s] 34%|███▍      | 100/294 [00:04<00:07, 25.43it/s] 35%|███▌      | 103/294 [00:04<00:07, 25.49it/s] 36%|███▌      | 106/294 [00:04<00:07, 25.54it/s] 37%|███▋      | 109/294 [00:04<00:07, 25.57it/s] 38%|███▊      | 112/294 [00:05<00:07, 25.54it/s] 39%|███▉      | 115/294 [00:05<00:07, 25.56it/s] 40%|████      | 118/294 [00:05<00:06, 25.59it/s] 41%|████      | 121/294 [00:05<00:06, 25.60it/s] 42%|████▏     | 124/294 [00:05<00:06, 25.61it/s] 43%|████▎     | 127/294 [00:05<00:06, 25.62it/s] 44%|████▍     | 130/294 [00:05<00:06, 25.63it/s] 45%|████▌     | 133/294 [00:05<00:06, 25.63it/s] 46%|████▋     | 136/294 [00:05<00:06, 25.63it/s] 47%|████▋     | 139/294 [00:06<00:06, 25.63it/s] 48%|████▊     | 142/294 [00:06<00:05, 25.63it/s] 49%|████▉     | 145/294 [00:06<00:05, 25.63it/s] 50%|█████     | 148/294 [00:06<00:05, 25.58it/s] 51%|█████▏    | 151/294 [00:06<00:05, 25.60it/s] 52%|█████▏    | 154/294 [00:06<00:05, 25.60it/s] 53%|█████▎    | 157/294 [00:06<00:05, 25.61it/s] 54%|█████▍    | 160/294 [00:06<00:05, 25.63it/s] 55%|█████▌    | 163/294 [00:07<00:05, 25.63it/s] 56%|█████▋    | 166/294 [00:07<00:04, 25.64it/s] 57%|█████▋    | 169/294 [00:07<00:04, 25.64it/s] 59%|█████▊    | 172/294 [00:07<00:04, 25.64it/s] 60%|█████▉    | 175/294 [00:07<00:04, 25.64it/s] 61%|██████    | 178/294 [00:07<00:04, 25.64it/s] 62%|██████▏   | 181/294 [00:07<00:04, 25.64it/s] 63%|██████▎   | 184/294 [00:07<00:04, 25.64it/s] 64%|██████▎   | 187/294 [00:07<00:04, 25.64it/s] 65%|██████▍   | 190/294 [00:08<00:04, 25.64it/s] 66%|██████▌   | 193/294 [00:08<00:03, 25.63it/s] 67%|██████▋   | 196/294 [00:08<00:03, 25.79it/s] 68%|██████▊   | 199/294 [00:08<00:03, 25.74it/s] 69%|██████▊   | 202/294 [00:08<00:03, 25.71it/s] 70%|██████▉   | 205/294 [00:08<00:03, 25.69it/s] 71%|███████   | 208/294 [00:08<00:03, 25.68it/s] 72%|███████▏  | 211/294 [00:08<00:03, 25.67it/s] 73%|███████▎  | 214/294 [00:08<00:03, 25.66it/s] 74%|███████▍  | 217/294 [00:09<00:03, 25.66it/s] 75%|███████▍  | 220/294 [00:09<00:02, 25.66it/s] 76%|███████▌  | 223/294 [00:09<00:02, 25.65it/s] 77%|███████▋  | 226/294 [00:09<00:02, 25.65it/s] 78%|███████▊  | 229/294 [00:09<00:02, 25.64it/s] 79%|███████▉  | 232/294 [00:09<00:02, 25.64it/s] 80%|███████▉  | 235/294 [00:09<00:02, 25.64it/s] 81%|████████  | 238/294 [00:09<00:02, 25.64it/s] 82%|████████▏ | 241/294 [00:10<00:02, 25.64it/s] 83%|████████▎ | 244/294 [00:10<00:01, 25.64it/s] 84%|████████▍ | 247/294 [00:10<00:01, 25.65it/s] 85%|████████▌ | 250/294 [00:10<00:01, 25.65it/s] 86%|████████▌ | 253/294 [00:10<00:01, 25.66it/s] 87%|████████▋ | 256/294 [00:10<00:01, 25.66it/s] 88%|████████▊ | 259/294 [00:10<00:01, 25.66it/s] 89%|████████▉ | 262/294 [00:10<00:01, 25.66it/s] 90%|█████████ | 265/294 [00:10<00:01, 25.65it/s] 91%|█████████ | 268/294 [00:11<00:01, 25.65it/s] 92%|█████████▏| 271/294 [00:11<00:00, 25.65it/s] 93%|█████████▎| 274/294 [00:11<00:00, 25.65it/s] 94%|█████████▍| 277/294 [00:11<00:00, 25.65it/s] 95%|█████████▌| 280/294 [00:11<00:00, 25.61it/s] 96%|█████████▋| 283/294 [00:11<00:00, 25.62it/s] 97%|█████████▋| 286/294 [00:11<00:00, 25.63it/s] 98%|█████████▊| 289/294 [00:11<00:00, 25.64it/s] 99%|█████████▉| 292/294 [00:12<00:00, 25.64it/s]                                                 100%|██████████| 294/294 [00:12<00:00, 25.64it/s]100%|██████████| 294/294 [00:12<00:00, 24.28it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.116, 'train_samples_per_second': 192.637, 'train_steps_per_second': 24.265, 'train_loss': 0.5759758981717688, 'epoch': 3.0}
Evaluation
2023-05-24 09:51:55.694 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:51:55.694 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:51:55.694 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/normal/dev.tsv
2023-05-24 09:51:55.697 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:51:55.698 | INFO     | __main__:<module>:108 - 0.88 0.89 0.82 0.94 0.82 0.89
Evaluation
2023-05-24 09:51:56.278 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:51:56.278 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:51:56.279 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/normal/test.tsv
2023-05-24 09:51:56.282 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:51:56.282 | INFO     | __main__:<module>:108 - 0.85 0.85 0.83 0.89 0.83 0.85
duygusal
undersampling
Testing turkish , augmentation: duygusal
2023-05-24 09:51:57.723056: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:51:57.761482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:51:58.340837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:51:58.859 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f9b3adfd700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:30,  1.42it/s]  1%|▏         | 4/300 [00:00<00:49,  6.01it/s]  2%|▏         | 7/300 [00:00<00:28, 10.16it/s]  3%|▎         | 10/300 [00:01<00:21, 13.74it/s]  4%|▍         | 13/300 [00:01<00:17, 16.66it/s]  5%|▌         | 16/300 [00:01<00:14, 18.97it/s]  6%|▋         | 19/300 [00:01<00:13, 20.72it/s]  7%|▋         | 22/300 [00:01<00:12, 22.03it/s]  8%|▊         | 25/300 [00:01<00:11, 23.00it/s]  9%|▉         | 28/300 [00:01<00:11, 23.65it/s] 10%|█         | 31/300 [00:01<00:11, 24.16it/s] 11%|█▏        | 34/300 [00:02<00:10, 24.53it/s] 12%|█▏        | 37/300 [00:02<00:10, 24.78it/s] 13%|█▎        | 40/300 [00:02<00:10, 24.97it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.09it/s] 15%|█▌        | 46/300 [00:02<00:10, 25.13it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.21it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.26it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.30it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.33it/s] 20%|██        | 61/300 [00:03<00:09, 25.35it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.36it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.33it/s] 23%|██▎       | 70/300 [00:03<00:09, 25.35it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.36it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.36it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.37it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.38it/s] 28%|██▊       | 85/300 [00:04<00:08, 25.38it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.34it/s] 30%|███       | 91/300 [00:04<00:08, 25.35it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.36it/s] 32%|███▏      | 97/300 [00:04<00:08, 25.37it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.38it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.38it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.33it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.35it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.36it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.37it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.38it/s] 40%|████      | 121/300 [00:05<00:07, 25.38it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.38it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.34it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.36it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.37it/s] 45%|████▌     | 136/300 [00:06<00:06, 25.38it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.38it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.38it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.38it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.34it/s] 50%|█████     | 151/300 [00:06<00:05, 25.36it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.37it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.37it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.38it/s] 54%|█████▍    | 163/300 [00:07<00:05, 25.38it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.38it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.39it/s] 57%|█████▋    | 172/300 [00:07<00:05, 25.39it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.40it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.39it/s] 60%|██████    | 181/300 [00:07<00:04, 25.39it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.39it/s] 62%|██████▏   | 187/300 [00:08<00:04, 25.39it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.39it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.39it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.38it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.39it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.39it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.39it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.37it/s] 70%|███████   | 211/300 [00:08<00:03, 25.38it/s] 71%|███████▏  | 214/300 [00:09<00:03, 25.38it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.39it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.38it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.38it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.33it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.35it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.36it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.37it/s] 79%|███████▉  | 238/300 [00:10<00:02, 25.36it/s] 80%|████████  | 241/300 [00:10<00:02, 25.37it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.37it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.34it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.35it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.36it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.37it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.37it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.38it/s] 88%|████████▊ | 265/300 [00:11<00:01, 25.38it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.38it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.38it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.38it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.38it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.38it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.39it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.39it/s] 96%|█████████▋| 289/300 [00:12<00:00, 25.39it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.39it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.39it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.39it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.39it/s]100%|██████████| 300/300 [00:12<00:00, 24.00it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.5045, 'train_samples_per_second': 191.931, 'train_steps_per_second': 23.991, 'train_loss': 0.5853075154622396, 'epoch': 3.0}
Evaluation
2023-05-24 09:52:30.581 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:52:30.582 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:52:30.582 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/duygusal/dev.tsv
2023-05-24 09:52:30.585 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:52:30.585 | INFO     | __main__:<module>:108 - 0.88 0.88 0.88 0.88 0.88 0.88
Evaluation
2023-05-24 09:52:31.178 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:52:31.179 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:52:31.179 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/duygusal/test.tsv
2023-05-24 09:52:31.183 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:52:31.183 | INFO     | __main__:<module>:108 - 0.86 0.86 0.87 0.87 0.87 0.86
propaganda
undersampling
Testing turkish , augmentation: propaganda
2023-05-24 09:52:32.570606: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:52:32.609163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:52:33.193000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:52:33.710 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f01d2866700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:23,  1.47it/s]  1%|▏         | 4/300 [00:00<00:47,  6.17it/s]  2%|▏         | 7/300 [00:00<00:28, 10.38it/s]  3%|▎         | 10/300 [00:01<00:20, 13.97it/s]  4%|▍         | 13/300 [00:01<00:16, 16.89it/s]  5%|▌         | 16/300 [00:01<00:14, 19.17it/s]  6%|▋         | 19/300 [00:01<00:13, 20.91it/s]  7%|▋         | 22/300 [00:01<00:12, 22.19it/s]  8%|▊         | 25/300 [00:01<00:11, 23.14it/s]  9%|▉         | 28/300 [00:01<00:11, 23.77it/s] 10%|█         | 31/300 [00:01<00:11, 24.27it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.63it/s] 12%|█▏        | 37/300 [00:02<00:10, 24.87it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.05it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.17it/s] 15%|█▌        | 46/300 [00:02<00:10, 25.26it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.27it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.33it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.37it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.40it/s] 20%|██        | 61/300 [00:03<00:09, 25.42it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.43it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.40it/s] 23%|██▎       | 70/300 [00:03<00:09, 25.42it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.44it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.44it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.45it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.45it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.45it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.40it/s] 30%|███       | 91/300 [00:04<00:08, 25.42it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.44it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.45it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.45it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.46it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.47it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.42it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.44it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.45it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.45it/s] 40%|████      | 121/300 [00:05<00:07, 25.46it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.47it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.43it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.44it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.45it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.46it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.45it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.46it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.46it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.45it/s] 50%|█████     | 151/300 [00:06<00:05, 25.45it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.46it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.46it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.46it/s] 54%|█████▍    | 163/300 [00:07<00:05, 25.46it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.46it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.46it/s] 57%|█████▋    | 172/300 [00:07<00:05, 25.46it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.46it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.46it/s] 60%|██████    | 181/300 [00:07<00:04, 25.47it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.47it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.48it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.47it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.47it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.47it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.47it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.47it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.47it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.47it/s] 70%|███████   | 211/300 [00:08<00:03, 25.47it/s] 71%|███████▏  | 214/300 [00:09<00:03, 25.48it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.47it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.47it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.47it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.47it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.47it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.47it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.47it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.46it/s] 80%|████████  | 241/300 [00:10<00:02, 25.47it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.47it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.47it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.48it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.48it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.48it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.48it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.48it/s] 88%|████████▊ | 265/300 [00:11<00:01, 25.47it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.47it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.46it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.46it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.47it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.47it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.47it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.47it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.47it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.47it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.47it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.47it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.47it/s]100%|██████████| 300/300 [00:12<00:00, 24.12it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.442, 'train_samples_per_second': 192.895, 'train_steps_per_second': 24.112, 'train_loss': 0.5894675699869791, 'epoch': 3.0}
Evaluation
2023-05-24 09:53:05.450 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:53:05.450 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:53:05.450 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/propaganda/dev.tsv
2023-05-24 09:53:05.454 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:53:05.454 | INFO     | __main__:<module>:108 - 0.90 0.91 0.93 0.89 0.93 0.91
Evaluation
2023-05-24 09:53:06.004 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:53:06.005 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:53:06.005 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/propaganda/test.tsv
2023-05-24 09:53:06.008 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:53:06.008 | INFO     | __main__:<module>:108 - 0.86 0.86 0.91 0.84 0.91 0.86
öznel
undersampling
Testing turkish , augmentation: öznel
2023-05-24 09:53:07.434041: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:53:07.472473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:53:08.052763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:53:08.567 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7fb45586f700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:19,  1.50it/s]  1%|▏         | 4/300 [00:00<00:47,  6.27it/s]  2%|▏         | 7/300 [00:00<00:27, 10.55it/s]  3%|▎         | 10/300 [00:01<00:20, 14.18it/s]  4%|▍         | 13/300 [00:01<00:16, 17.11it/s]  5%|▌         | 16/300 [00:01<00:14, 19.40it/s]  6%|▋         | 19/300 [00:01<00:13, 21.09it/s]  7%|▋         | 22/300 [00:01<00:12, 22.39it/s]  8%|▊         | 25/300 [00:01<00:11, 23.34it/s]  9%|▉         | 28/300 [00:01<00:11, 24.03it/s] 10%|█         | 31/300 [00:01<00:10, 24.52it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.87it/s] 12%|█▏        | 37/300 [00:02<00:10, 25.11it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.29it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.41it/s] 15%|█▌        | 46/300 [00:02<00:09, 25.50it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.56it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.60it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.63it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.65it/s] 20%|██        | 61/300 [00:03<00:09, 25.62it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.64it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.66it/s] 23%|██▎       | 70/300 [00:03<00:08, 25.67it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.67it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.67it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.68it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.68it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.68it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.69it/s] 30%|███       | 91/300 [00:04<00:08, 25.70it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.70it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.70it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.70it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.69it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.70it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.70it/s] 37%|███▋      | 112/300 [00:04<00:07, 25.69it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.69it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.70it/s] 40%|████      | 121/300 [00:05<00:06, 25.69it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.70it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.70it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.70it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.70it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.68it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.67it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.59it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.61it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.62it/s] 50%|█████     | 151/300 [00:06<00:05, 25.62it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.62it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.63it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.60it/s] 54%|█████▍    | 163/300 [00:06<00:05, 25.62it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.65it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.67it/s] 57%|█████▋    | 172/300 [00:07<00:04, 25.68it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.68it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.69it/s] 60%|██████    | 181/300 [00:07<00:04, 25.69it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.70it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.70it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.70it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.70it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.70it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.70it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.66it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.67it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.68it/s] 70%|███████   | 211/300 [00:08<00:03, 25.69it/s] 71%|███████▏  | 214/300 [00:08<00:03, 25.70it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.70it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.70it/s] 74%|███████▍  | 223/300 [00:09<00:02, 25.70it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.70it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.70it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.70it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.70it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.70it/s] 80%|████████  | 241/300 [00:10<00:02, 25.70it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.71it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.70it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.70it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.70it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.70it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.70it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.65it/s] 88%|████████▊ | 265/300 [00:10<00:01, 25.67it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.67it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.68it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.69it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.70it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.70it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.71it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.71it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.71it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.71it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.71it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.70it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.70it/s]100%|██████████| 300/300 [00:12<00:00, 24.35it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.3254, 'train_samples_per_second': 194.72, 'train_steps_per_second': 24.34, 'train_loss': 0.589406992594401, 'epoch': 3.0}
Evaluation
2023-05-24 09:53:40.228 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:53:40.228 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:53:40.228 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/öznel/dev.tsv
2023-05-24 09:53:40.232 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:53:40.232 | INFO     | __main__:<module>:108 - 0.89 0.89 0.84 0.93 0.84 0.89
Evaluation
2023-05-24 09:53:40.781 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:53:40.781 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:53:40.781 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/öznel/test.tsv
2023-05-24 09:53:40.785 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:53:40.785 | INFO     | __main__:<module>:108 - 0.84 0.84 0.87 0.84 0.87 0.84
abartılı
undersampling
Testing turkish , augmentation: abartılı
2023-05-24 09:53:42.221929: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:53:42.260481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:53:42.838166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:53:43.355 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f5729a00700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:20,  1.49it/s]  1%|▏         | 4/300 [00:00<00:47,  6.24it/s]  2%|▏         | 7/300 [00:00<00:27, 10.49it/s]  3%|▎         | 10/300 [00:01<00:20, 14.06it/s]  4%|▍         | 13/300 [00:01<00:16, 16.96it/s]  5%|▌         | 16/300 [00:01<00:14, 19.23it/s]  6%|▋         | 19/300 [00:01<00:13, 20.94it/s]  7%|▋         | 22/300 [00:01<00:12, 22.21it/s]  8%|▊         | 25/300 [00:01<00:11, 23.15it/s]  9%|▉         | 28/300 [00:01<00:11, 23.77it/s] 10%|█         | 31/300 [00:01<00:11, 24.26it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.61it/s] 12%|█▏        | 37/300 [00:02<00:10, 24.85it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.03it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.16it/s] 15%|█▌        | 46/300 [00:02<00:10, 25.24it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.25it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.31it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.35it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.38it/s] 20%|██        | 61/300 [00:03<00:09, 25.40it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.41it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.43it/s] 23%|██▎       | 70/300 [00:03<00:09, 25.38it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.40it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.41it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.42it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.43it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.43it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.44it/s] 30%|███       | 91/300 [00:04<00:08, 25.43it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.43it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.44it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.44it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.45it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.44it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.45it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.45it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.45it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.45it/s] 40%|████      | 121/300 [00:05<00:07, 25.45it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.45it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.45it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.41it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.42it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.43it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.44it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.44it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.44it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.41it/s] 50%|█████     | 151/300 [00:06<00:05, 25.42it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.43it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.43it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.44it/s] 54%|█████▍    | 163/300 [00:07<00:05, 25.45it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.45it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.45it/s] 57%|█████▋    | 172/300 [00:07<00:05, 25.45it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.45it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.45it/s] 60%|██████    | 181/300 [00:07<00:04, 25.45it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.46it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.45it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.45it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.45it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.45it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.45it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.45it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.45it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.45it/s] 70%|███████   | 211/300 [00:08<00:03, 25.45it/s] 71%|███████▏  | 214/300 [00:09<00:03, 25.45it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.45it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.45it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.45it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.45it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.44it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.44it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.44it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.44it/s] 80%|████████  | 241/300 [00:10<00:02, 25.43it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.43it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.44it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.44it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.44it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.45it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.45it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.46it/s] 88%|████████▊ | 265/300 [00:11<00:01, 25.45it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.45it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.43it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.44it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.45it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.45it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.46it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.46it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.42it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.43it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.44it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.45it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.45it/s]100%|██████████| 300/300 [00:12<00:00, 24.12it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.4413, 'train_samples_per_second': 192.905, 'train_steps_per_second': 24.113, 'train_loss': 0.5717291768391927, 'epoch': 3.0}
Evaluation
2023-05-24 09:54:13.986 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:54:13.986 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:54:13.986 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/abartılı/dev.tsv
2023-05-24 09:54:13.990 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:54:13.990 | INFO     | __main__:<module>:108 - 0.88 0.89 0.80 0.95 0.80 0.88
Evaluation
2023-05-24 09:54:14.573 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:54:14.573 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:54:14.573 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/abartılı/test.tsv
2023-05-24 09:54:14.577 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:54:14.577 | INFO     | __main__:<module>:108 - 0.84 0.84 0.84 0.86 0.84 0.84
aşağılayıcı
undersampling
Testing turkish , augmentation: aşağılayıcı
2023-05-24 09:54:16.004054: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:54:16.042261: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:54:16.622240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:54:17.138 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f7663367700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:22,  1.48it/s]  1%|▏         | 4/300 [00:00<00:47,  6.20it/s]  2%|▏         | 7/300 [00:00<00:28, 10.45it/s]  3%|▎         | 10/300 [00:01<00:20, 14.06it/s]  4%|▍         | 13/300 [00:01<00:16, 16.98it/s]  5%|▌         | 16/300 [00:01<00:14, 19.27it/s]  6%|▋         | 19/300 [00:01<00:13, 21.00it/s]  7%|▋         | 22/300 [00:01<00:12, 22.29it/s]  8%|▊         | 25/300 [00:01<00:11, 23.23it/s]  9%|▉         | 28/300 [00:01<00:11, 23.89it/s] 10%|█         | 31/300 [00:01<00:11, 24.38it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.73it/s] 12%|█▏        | 37/300 [00:02<00:10, 24.97it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.14it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.26it/s] 15%|█▌        | 46/300 [00:02<00:10, 25.32it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.39it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.44it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.48it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.51it/s] 20%|██        | 61/300 [00:03<00:09, 25.52it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.53it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.54it/s] 23%|██▎       | 70/300 [00:03<00:09, 25.55it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.55it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.55it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.55it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.56it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.56it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.52it/s] 30%|███       | 91/300 [00:04<00:08, 25.53it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.53it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.54it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.54it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.54it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.55it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.55it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.55it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.55it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.56it/s] 40%|████      | 121/300 [00:05<00:07, 25.57it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.57it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.57it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.55it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.53it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.52it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.52it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.51it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.54it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.55it/s] 50%|█████     | 151/300 [00:06<00:05, 25.55it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.56it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.56it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.56it/s] 54%|█████▍    | 163/300 [00:07<00:05, 25.56it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.55it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.50it/s] 57%|█████▋    | 172/300 [00:07<00:05, 25.52it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.53it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.54it/s] 60%|██████    | 181/300 [00:07<00:04, 25.54it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.55it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.55it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.55it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.55it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.55it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.54it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.56it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.56it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.53it/s] 70%|███████   | 211/300 [00:08<00:03, 25.56it/s] 71%|███████▏  | 214/300 [00:09<00:03, 25.57it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.57it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.58it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.59it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.59it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.59it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.59it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.59it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.59it/s] 80%|████████  | 241/300 [00:10<00:02, 25.58it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.58it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.56it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.59it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.59it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.59it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.59it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.59it/s] 88%|████████▊ | 265/300 [00:11<00:01, 25.59it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.54it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.56it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.56it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.57it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.58it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.58it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.59it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.54it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.56it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.57it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.58it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.58it/s]100%|██████████| 300/300 [00:12<00:00, 24.22it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.392, 'train_samples_per_second': 193.673, 'train_steps_per_second': 24.209, 'train_loss': 0.591221211751302, 'epoch': 3.0}
Evaluation
2023-05-24 09:54:47.646 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:54:47.646 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:54:47.646 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/aşağılayıcı/dev.tsv
2023-05-24 09:54:47.650 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:54:47.650 | INFO     | __main__:<module>:108 - 0.87 0.88 0.81 0.93 0.81 0.88
Evaluation
2023-05-24 09:54:48.205 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:54:48.205 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:54:48.205 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/aşağılayıcı/test.tsv
2023-05-24 09:54:48.209 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:54:48.209 | INFO     | __main__:<module>:108 - 0.84 0.85 0.86 0.85 0.86 0.85
partizan
undersampling
Testing turkish , augmentation: partizan
2023-05-24 09:54:49.638486: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:54:49.676885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:54:50.257447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:54:50.773 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f7e86a04700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:19,  1.50it/s]  1%|▏         | 4/300 [00:00<00:47,  6.26it/s]  2%|▏         | 7/300 [00:00<00:27, 10.53it/s]  3%|▎         | 10/300 [00:01<00:20, 14.15it/s]  4%|▍         | 13/300 [00:01<00:16, 17.09it/s]  5%|▌         | 16/300 [00:01<00:14, 19.37it/s]  6%|▋         | 19/300 [00:01<00:13, 21.10it/s]  7%|▋         | 22/300 [00:01<00:12, 22.34it/s]  8%|▊         | 25/300 [00:01<00:11, 23.29it/s]  9%|▉         | 28/300 [00:01<00:11, 23.97it/s] 10%|█         | 31/300 [00:01<00:10, 24.46it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.82it/s] 12%|█▏        | 37/300 [00:02<00:10, 25.06it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.23it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.31it/s] 15%|█▌        | 46/300 [00:02<00:09, 25.41it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.48it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.54it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.58it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.60it/s] 20%|██        | 61/300 [00:03<00:09, 25.62it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.63it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.64it/s] 23%|██▎       | 70/300 [00:03<00:08, 25.64it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.64it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.64it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.64it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.60it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.62it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.63it/s] 30%|███       | 91/300 [00:04<00:08, 25.63it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.64it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.64it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.65it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.65it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.66it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.66it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.65it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.65it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.65it/s] 40%|████      | 121/300 [00:05<00:06, 25.65it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.63it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.64it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.65it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.65it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.65it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.65it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.60it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.66it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.66it/s] 50%|█████     | 151/300 [00:06<00:05, 25.66it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.65it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.65it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.65it/s] 54%|█████▍    | 163/300 [00:06<00:05, 25.62it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.63it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.63it/s] 57%|█████▋    | 172/300 [00:07<00:04, 25.64it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.64it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.64it/s] 60%|██████    | 181/300 [00:07<00:04, 25.64it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.60it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.60it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.61it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.62it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.63it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.63it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.63it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.60it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.62it/s] 70%|███████   | 211/300 [00:08<00:03, 25.63it/s] 71%|███████▏  | 214/300 [00:08<00:03, 25.64it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.64it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.64it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.65it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.65it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.64it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.64it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.64it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.64it/s] 80%|████████  | 241/300 [00:10<00:02, 25.64it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.65it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.65it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.66it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.66it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.66it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.66it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.67it/s] 88%|████████▊ | 265/300 [00:10<00:01, 25.66it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.66it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.66it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.66it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.67it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.67it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.68it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.68it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.68it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.67it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.67it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.68it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.68it/s]100%|██████████| 300/300 [00:12<00:00, 24.31it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[B^[[B^[[A^[[A/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.3452, 'train_samples_per_second': 194.407, 'train_steps_per_second': 24.301, 'train_loss': 0.5817695109049479, 'epoch': 3.0}
Evaluation
2023-05-24 09:55:21.562 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:55:21.562 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:55:21.562 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/partizan/dev.tsv
2023-05-24 09:55:21.565 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:55:21.566 | INFO     | __main__:<module>:108 - 0.90 0.90 0.87 0.93 0.87 0.90
Evaluation
2023-05-24 09:55:22.122 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:55:22.122 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:55:22.123 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/partizan/test.tsv
2023-05-24 09:55:22.126 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:55:22.126 | INFO     | __main__:<module>:108 - 0.84 0.84 0.86 0.85 0.86 0.84
önyargılı
undersampling
Testing turkish , augmentation: önyargılı
2023-05-24 09:55:23.556379: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-05-24 09:55:23.594719: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-24 09:55:24.176925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-05-24 09:55:24.694 | INFO     | app.models.transformer_models:train:110 - <pandas.core.groupby.generic.SeriesGroupBy object at 0x7f7ebf5d9700>
Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/web/miniconda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<03:18,  1.51it/s]  1%|▏         | 4/300 [00:00<00:46,  6.30it/s]  2%|▏         | 7/300 [00:00<00:27, 10.58it/s]  3%|▎         | 10/300 [00:01<00:20, 14.19it/s]  4%|▍         | 13/300 [00:01<00:16, 17.07it/s]  5%|▌         | 16/300 [00:01<00:14, 19.34it/s]  6%|▋         | 19/300 [00:01<00:13, 21.06it/s]  7%|▋         | 22/300 [00:01<00:12, 22.34it/s]  8%|▊         | 25/300 [00:01<00:11, 23.26it/s]  9%|▉         | 28/300 [00:01<00:11, 23.93it/s] 10%|█         | 31/300 [00:01<00:11, 24.42it/s] 11%|█▏        | 34/300 [00:01<00:10, 24.76it/s] 12%|█▏        | 37/300 [00:02<00:10, 25.00it/s] 13%|█▎        | 40/300 [00:02<00:10, 25.17it/s] 14%|█▍        | 43/300 [00:02<00:10, 25.29it/s] 15%|█▌        | 46/300 [00:02<00:10, 25.34it/s] 16%|█▋        | 49/300 [00:02<00:09, 25.44it/s] 17%|█▋        | 52/300 [00:02<00:09, 25.48it/s] 18%|█▊        | 55/300 [00:02<00:09, 25.51it/s] 19%|█▉        | 58/300 [00:02<00:09, 25.53it/s] 20%|██        | 61/300 [00:03<00:09, 25.54it/s] 21%|██▏       | 64/300 [00:03<00:09, 25.55it/s] 22%|██▏       | 67/300 [00:03<00:09, 25.56it/s] 23%|██▎       | 70/300 [00:03<00:08, 25.56it/s] 24%|██▍       | 73/300 [00:03<00:08, 25.56it/s] 25%|██▌       | 76/300 [00:03<00:08, 25.56it/s] 26%|██▋       | 79/300 [00:03<00:08, 25.56it/s] 27%|██▋       | 82/300 [00:03<00:08, 25.55it/s] 28%|██▊       | 85/300 [00:03<00:08, 25.55it/s] 29%|██▉       | 88/300 [00:04<00:08, 25.55it/s] 30%|███       | 91/300 [00:04<00:08, 25.56it/s] 31%|███▏      | 94/300 [00:04<00:08, 25.56it/s] 32%|███▏      | 97/300 [00:04<00:07, 25.56it/s] 33%|███▎      | 100/300 [00:04<00:07, 25.56it/s] 34%|███▍      | 103/300 [00:04<00:07, 25.56it/s] 35%|███▌      | 106/300 [00:04<00:07, 25.56it/s] 36%|███▋      | 109/300 [00:04<00:07, 25.57it/s] 37%|███▋      | 112/300 [00:05<00:07, 25.56it/s] 38%|███▊      | 115/300 [00:05<00:07, 25.56it/s] 39%|███▉      | 118/300 [00:05<00:07, 25.56it/s] 40%|████      | 121/300 [00:05<00:07, 25.56it/s] 41%|████▏     | 124/300 [00:05<00:06, 25.56it/s] 42%|████▏     | 127/300 [00:05<00:06, 25.56it/s] 43%|████▎     | 130/300 [00:05<00:06, 25.56it/s] 44%|████▍     | 133/300 [00:05<00:06, 25.56it/s] 45%|████▌     | 136/300 [00:05<00:06, 25.56it/s] 46%|████▋     | 139/300 [00:06<00:06, 25.56it/s] 47%|████▋     | 142/300 [00:06<00:06, 25.56it/s] 48%|████▊     | 145/300 [00:06<00:06, 25.57it/s] 49%|████▉     | 148/300 [00:06<00:05, 25.52it/s] 50%|█████     | 151/300 [00:06<00:05, 25.53it/s] 51%|█████▏    | 154/300 [00:06<00:05, 25.53it/s] 52%|█████▏    | 157/300 [00:06<00:05, 25.53it/s] 53%|█████▎    | 160/300 [00:06<00:05, 25.54it/s] 54%|█████▍    | 163/300 [00:07<00:05, 25.55it/s] 55%|█████▌    | 166/300 [00:07<00:05, 25.56it/s] 56%|█████▋    | 169/300 [00:07<00:05, 25.56it/s] 57%|█████▋    | 172/300 [00:07<00:05, 25.56it/s] 58%|█████▊    | 175/300 [00:07<00:04, 25.56it/s] 59%|█████▉    | 178/300 [00:07<00:04, 25.56it/s] 60%|██████    | 181/300 [00:07<00:04, 25.56it/s] 61%|██████▏   | 184/300 [00:07<00:04, 25.56it/s] 62%|██████▏   | 187/300 [00:07<00:04, 25.56it/s] 63%|██████▎   | 190/300 [00:08<00:04, 25.56it/s] 64%|██████▍   | 193/300 [00:08<00:04, 25.56it/s] 65%|██████▌   | 196/300 [00:08<00:04, 25.55it/s] 66%|██████▋   | 199/300 [00:08<00:03, 25.56it/s] 67%|██████▋   | 202/300 [00:08<00:03, 25.56it/s] 68%|██████▊   | 205/300 [00:08<00:03, 25.56it/s] 69%|██████▉   | 208/300 [00:08<00:03, 25.56it/s] 70%|███████   | 211/300 [00:08<00:03, 25.56it/s] 71%|███████▏  | 214/300 [00:09<00:03, 25.51it/s] 72%|███████▏  | 217/300 [00:09<00:03, 25.53it/s] 73%|███████▎  | 220/300 [00:09<00:03, 25.54it/s] 74%|███████▍  | 223/300 [00:09<00:03, 25.55it/s] 75%|███████▌  | 226/300 [00:09<00:02, 25.55it/s] 76%|███████▋  | 229/300 [00:09<00:02, 25.55it/s] 77%|███████▋  | 232/300 [00:09<00:02, 25.55it/s] 78%|███████▊  | 235/300 [00:09<00:02, 25.55it/s] 79%|███████▉  | 238/300 [00:09<00:02, 25.55it/s] 80%|████████  | 241/300 [00:10<00:02, 25.55it/s] 81%|████████▏ | 244/300 [00:10<00:02, 25.56it/s] 82%|████████▏ | 247/300 [00:10<00:02, 25.51it/s] 83%|████████▎ | 250/300 [00:10<00:01, 25.53it/s] 84%|████████▍ | 253/300 [00:10<00:01, 25.54it/s] 85%|████████▌ | 256/300 [00:10<00:01, 25.55it/s] 86%|████████▋ | 259/300 [00:10<00:01, 25.55it/s] 87%|████████▋ | 262/300 [00:10<00:01, 25.56it/s] 88%|████████▊ | 265/300 [00:10<00:01, 25.56it/s] 89%|████████▉ | 268/300 [00:11<00:01, 25.56it/s] 90%|█████████ | 271/300 [00:11<00:01, 25.56it/s] 91%|█████████▏| 274/300 [00:11<00:01, 25.56it/s] 92%|█████████▏| 277/300 [00:11<00:00, 25.56it/s] 93%|█████████▎| 280/300 [00:11<00:00, 25.56it/s] 94%|█████████▍| 283/300 [00:11<00:00, 25.56it/s] 95%|█████████▌| 286/300 [00:11<00:00, 25.56it/s] 96%|█████████▋| 289/300 [00:11<00:00, 25.56it/s] 97%|█████████▋| 292/300 [00:12<00:00, 25.56it/s] 98%|█████████▊| 295/300 [00:12<00:00, 25.56it/s] 99%|█████████▉| 298/300 [00:12<00:00, 25.56it/s]                                                 100%|██████████| 300/300 [00:12<00:00, 25.56it/s]100%|██████████| 300/300 [00:12<00:00, 24.25it/s]
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/web/miniconda/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
{'train_runtime': 12.3802, 'train_samples_per_second': 193.858, 'train_steps_per_second': 24.232, 'train_loss': 0.5894218444824219, 'epoch': 3.0}
Evaluation
2023-05-24 09:55:55.546 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:55:55.546 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:55:55.547 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/önyargılı/dev.tsv
2023-05-24 09:55:55.550 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:55:55.550 | INFO     | __main__:<module>:108 - 0.89 0.90 0.83 0.94 0.83 0.89
Evaluation
2023-05-24 09:55:56.129 | INFO     | __main__:validate_files:40 - The file is properly formatted
2023-05-24 09:55:56.129 | INFO     | __main__:<module>:89 - Started evaluating results for task-2...
2023-05-24 09:55:56.130 | INFO     | __main__:<module>:91 - results/dbmdz/bert-base-turkish-128k-cased/turkish/önyargılı/test.tsv
2023-05-24 09:55:56.133 | INFO     | __main__:<module>:97 - Macro F1 Macro P Macro R SUBJ F1 SUBJ P SUBJ R Accuracy
2023-05-24 09:55:56.133 | INFO     | __main__:<module>:108 - 0.84 0.84 0.85 0.85 0.85 0.84
